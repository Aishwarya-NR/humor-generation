{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import importlib\n",
    "from library import data_preprocess as dp\n",
    "importlib.reload(dp)\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras import regularizers\n",
    "from keras.metrics import sparse_categorical_accuracy, sparse_categorical_crossentropy\n",
    "\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './datasets/jokes.pickle'\n",
    "VOCAB_PATH = './datasets/jokes_vocabulary.pickle'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 1028\n",
    "DROPOUT_FACTOR = 0.333\n",
    "REGULARIZATION = 0.00001\n",
    "\n",
    "DATA_PERCENT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences =  96910\n",
      "[\"sos how many nazi's does it take to screw in a lightbulb ? nein eos\", \"sos interviewer to me : what is your weakness ? i replied : honesty . interviewer : but honesty is not a weakness that's a good thing . i replied : i do not give a fuck what you think . eos\"]\n",
      "Vocab size =  8922\n",
      "['sos', 'did', 'you', 'hear', 'about', 'the', 'new', 'corduroy', 'pillows', '?']\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as pickleFile:\n",
    "    sentences = pickle.load(pickleFile)\n",
    "\n",
    "with open(VOCAB_PATH, 'rb') as pickleFile:\n",
    "    vocab = pickle.load(pickleFile)\n",
    "    \n",
    "random.shuffle(sentences)\n",
    "\n",
    "print(\"Number of sentences = \", len(sentences))\n",
    "print(sentences[:2])\n",
    "print(\"Vocab size = \", len(vocab))\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 32, 112, 8025, 64, 16, 83, 8, 352, 14, 4, 322, 6, 3071, 2], [1, 2178, 8, 22, 23, 17, 18, 35, 3470, 6, 7, 672, 23, 3927, 3, 2178, 23, 41, 3927, 18, 58, 4, 3470, 156, 4, 109, 146, 3, 7, 672, 23, 7, 20, 58, 181, 4, 261, 17, 9, 106, 3, 2], [1, 17, 20, 9, 66, 85, 211, 29, 62, 591, 12, 62, 303, 1254, 147, 4, 682, 6, 4454, 12, 5353, 2], [1, 45, 58, 388, 41, 15, 35, 412, 387, 7515, 3, 11, 32, 18, 28, 388, 6, 11, 7, 79, 7, 329, 388, 3, 9, 89, 941, 3, 8026, 316, 3, 2], [1, 4676, 30, 531, 23, 11, 39, 9, 537, 19, 2179, 29, 65, 9, 401, 67, 6, 11, 2]]\n",
      "8923\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "num_words = len(vocab)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, filters='', lower=True, split=' ', \n",
    "                      char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "assert num_words == len(tokenizer.word_index)\n",
    "\n",
    "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded_sentences[:5])\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training data size =  1073602\n",
      "Max seq len =  10\n",
      "(1073602, 10)\n",
      "[[   1   32  112 8025   64   16   83    8  352   14]\n",
      " [  32  112 8025   64   16   83    8  352   14    4]]\n",
      "(1073602, 1)\n",
      "[[  4]\n",
      " [322]]\n"
     ]
    }
   ],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "for sentence in encoded_sentences:\n",
    "    l = len(sentence)\n",
    "    sliding_window_length = min(l-3, MAX_SEQUENCE_LENGTH)\n",
    "    step_size = 1\n",
    "    for i in range(0, l - sliding_window_length, step_size):\n",
    "        X_data.append(sentence[i:i+sliding_window_length])\n",
    "        y_data.append(sentence[i+sliding_window_length])\n",
    "        \n",
    "print(\"Total training data size = \", len(X_data))\n",
    "MAX_SEQ_LEN = max([len(seq) for seq in X_data])\n",
    "print(\"Max seq len = \", MAX_SEQ_LEN)\n",
    "X_data = pad_sequences(X_data, maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "y_data = np.array(y_data).reshape(-1,1)\n",
    "print(X_data.shape)\n",
    "print(X_data[:2])\n",
    "print(y_data.shape)\n",
    "print(y_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def BiLSTM(vocab_size, embedding_dim, hidden_dim, max_seq_len, \n",
    "           dropout_factor=0.5, regularization=0.00001):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_seq_len, \n",
    "                        mask_zero=True, embeddings_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Bidirectional(LSTM(hidden_dim, \n",
    "                                 activation='elu',\n",
    "                                 kernel_regularizer=regularizers.l2(regularization),\n",
    "                                 recurrent_regularizer=regularizers.l2(regularization),\n",
    "                                 unroll=True\n",
    "                                )))\n",
    "    model.add(Dropout(dropout_factor))\n",
    "    model.add(Dense(units=vocab_size, activation='softmax', \n",
    "              kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n",
    "                  metrics=[sparse_categorical_crossentropy, sparse_categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 10, 256)           2284288   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 2056)              10567840  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2056)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8923)              18354611  \n",
      "=================================================================\n",
      "Total params: 31,206,739\n",
      "Trainable params: 31,206,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "              max_seq_len=MAX_SEQ_LEN, dropout_factor=DROPOUT_FACTOR, regularization=REGULARIZATION)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TB(TensorBoard):\n",
    "    def __init__(self, log_every=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.log_every = log_every\n",
    "        self.counter = 0\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.counter+=1\n",
    "        if self.counter%self.log_every==0:\n",
    "            for name, value in logs.items():\n",
    "                if name in ['batch', 'size']:\n",
    "                    continue\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.writer.add_summary(summary, self.counter)\n",
    "            self.writer.flush()\n",
    "        \n",
    "        super().on_batch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 858881 samples, validate on 214721 samples\n",
      "Epoch 1/5\n",
      "   704/858881 [..............................] - ETA: 10:13:02 - loss: 8.7217 - sparse_categorical_crossentropy: 8.6585 - sparse_categorical_accuracy: 0.0597"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tensorboard = TB(log_dir=\"./logs/jokes_bilstm/{}\".format(time()), \n",
    "                          histogram_freq=0, write_graph=True, write_images=False, log_every=10)\n",
    "\n",
    "callbacks=[tensorboard, \n",
    "           EarlyStopping(patience=4, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=MODELS_PATH + 'jokes_bilstm_gen.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=1)]\n",
    "\n",
    "model.fit(X_data, y_data, epochs=5, batch_size=64, shuffle=True, verbose=1, validation_split=0.2, \n",
    "          callbacks=callbacks)\n",
    "\n",
    "print(\"Total elapsed time: \", time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate(model, tokenizer, seed_text, maxlen):\n",
    "    \n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    print(seq)\n",
    "    \n",
    "    while True:\n",
    "        if len(seq) > MAX_SEQ_LEN:\n",
    "            encoded_seq = seq[-1*MAX_SEQ_LEN:]\n",
    "        else:\n",
    "            encoded_seq = seq\n",
    "        padded_seq = pad_sequences([encoded_seq], maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "        y_prob = model.predict(padded_seq)\n",
    "        y_class = y_prob.argmax(axis=-1)[0]\n",
    "        if y_class == 0:\n",
    "            break\n",
    "        out_word = reverse_word_map[y_class]\n",
    "        seq.append(y_class)\n",
    "        if out_word == 'eos' or len(seq) > maxlen:\n",
    "            break\n",
    "    \n",
    "    words = [reverse_word_map[idx] for idx in seq]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "sos the guy says \" i said , \" i don't know how i am . \" eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos\", maxlen=40)\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sos hello', 'hello ,', \", i'm\", \"i'm a\", 'a dinosaur', 'dinosaur .', '. eos']\n"
     ]
    }
   ],
   "source": [
    "def bigrams_list(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    bigrams = []\n",
    "    for i in range(0, len(words)-1):\n",
    "        bigrams.append(words[i]+' '+words[i+1])\n",
    "    return bigrams\n",
    "\n",
    "print(bigrams_list(\"sos hello , i'm a dinosaur . eos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sos how', 'how many', \"many nazi's\", \"nazi's does\", 'does it', 'it take', 'take to', 'to screw', 'screw in', 'in a', 'a lightbulb', 'lightbulb ?', '? nein', 'nein eos'], ['sos interviewer', 'interviewer to', 'to me', 'me :', ': what', 'what is', 'is your', 'your weakness', 'weakness ?', '? i', 'i replied', 'replied :', ': honesty', 'honesty .', '. interviewer', 'interviewer :', ': but', 'but honesty', 'honesty is', 'is not', 'not a', 'a weakness', \"weakness that's\", \"that's a\", 'a good', 'good thing', 'thing .', '. i', 'i replied', 'replied :', ': i', 'i do', 'do not', 'not give', 'give a', 'a fuck', 'fuck what', 'what you', 'you think', 'think .', '. eos']]\n"
     ]
    }
   ],
   "source": [
    "sentence_bigrams = [bigrams_list(s) for s in sentences]\n",
    "print(sentence_bigrams[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    temp = set(lst2)\n",
    "    lst3 = [value for value in lst1 if value in temp]\n",
    "    return lst3\n",
    "\n",
    "def similarity_score(lst1, lst2):\n",
    "    intersection_len = len(intersection(lst1, lst2))\n",
    "    return (1.0*intersection_len)/len(lst1)#+len(lst2)-intersection_len)\n",
    " \n",
    "def print_closest_sentences(sentence, sentence_bigrams, top_k=3):\n",
    "    bigrams = bigrams_list(sentence)\n",
    "    scores = np.array([similarity_score(bigrams, sbigrams)\n",
    "                       for sbigrams in sentence_bigrams])\n",
    "    top_k_indices = scores.argsort()[-1*top_k:][::-1]\n",
    "    top_k_scores = scores[top_k_indices]\n",
    "    for k in range(top_k):\n",
    "        print(top_k_scores[k], \" -> \", sentences[top_k_indices[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47058823529411764  ->  sos my friend asked me if her breath smelled like tacos . i said , \" i don't know , do you put shit in your tacos ? \" eos\n",
      "0.47058823529411764  ->  sos my girlfriend asked me the other day , \" dave , why do you always walk in front of me ? \" i said , \" i'm sorry , i don't follow you . \" eos\n",
      "0.47058823529411764  ->  sos i went to the library and asked if they had the book about tiny penises threedots the librarian said , \" i don't think it's in yet . \" i said , \" yes , that's the one . \" eos\n"
     ]
    }
   ],
   "source": [
    "print_closest_sentences(joke, sentence_bigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
