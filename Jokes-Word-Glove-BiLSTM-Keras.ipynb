{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import importlib\n",
    "from library import data_preprocess as dp\n",
    "importlib.reload(dp)\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras import regularizers\n",
    "from keras.metrics import sparse_categorical_accuracy, sparse_categorical_crossentropy\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './datasets/jokes.pickle'\n",
    "VOCAB_PATH = './datasets/jokes_vocabulary.pickle'\n",
    "MODELS_PATH = './models/'\n",
    "EMBEDDING_FILE = \"./data/glove/glove.6B.100d.txt\"\n",
    "MAX_SEQUENCE_LENGTH = 13\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "HIDDEN_DIM = 512\n",
    "DROPOUT_FACTOR = 0.333\n",
    "REGULARIZATION = 0.00001\n",
    "DEEPER_DIM = 256\n",
    "DATA_PERCENT = 0.1\n",
    "RUN_INDEX = 4\n",
    "\n",
    "# EMBEDDING_FILE = \"./data/glove/glove.6B.100d.txt\"\n",
    "# TRAIN_DATA_FILE = \"./datasets/combined.pickle\"\n",
    "# VOCABULARY_FILE = \"./datasets/combined_vocabulary.pickle\"\n",
    "# MAX_SEQUENCE_LENGTH = 10\n",
    "MAX_NB_WORDS = 200000\n",
    "# EMBEDDING_DIM = 100\n",
    "SEQUENCE_STEP = 1\n",
    "# #VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences =  96910\n",
      "['sos a man walks into an eye doctor and asks to see the doctor threedots the nurse replies , \" not with that eye ! \" eos', 'sos last night i got a handjob from a blind girl she said , \" you\\'ve got the biggest dick i\\'ve ever put my hands on . \" i said , \" nah , you\\'re just pulling my leg . \" eos']\n",
      "Vocab size =  8922\n",
      "['sos', 'did', 'you', 'hear', 'about', 'the', 'new', 'corduroy', 'pillows', '?']\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as pickleFile:\n",
    "    sentences = pickle.load(pickleFile)\n",
    "\n",
    "with open(VOCAB_PATH, 'rb') as pickleFile:\n",
    "    vocab = pickle.load(pickleFile)\n",
    "    \n",
    "random.shuffle(sentences)\n",
    "\n",
    "print(\"Number of sentences = \", len(sentences))\n",
    "print(sentences[:2])\n",
    "print(\"Vocab size = \", len(vocab))\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 73, 130, 72, 50, 466, 201, 12, 288, 8, 118, 5, 201, 15, 5, 1701, 650, 10, 11, 58, 29, 28, 466, 21, 11, 2], [1, 154, 168, 7, 77, 4, 4120, 78, 4, 298, 124, 70, 79, 10, 11, 395, 77, 5, 1032, 248, 153, 143, 148, 13, 384, 26, 3, 11, 7, 79, 10, 11, 1761, 10, 94, 44, 1984, 13, 564, 3, 11, 2], [1, 32, 20, 9, 67, 5, 3028, 24, 205, 14, 2366, 6, 1919, 498, 16, 2502, 166, 5, 11, 6156, 11, 2], [1, 13, 2537, 1254, 71, 24, 2701, 78, 1157, 1630, 259, 672, 31, 799, 31, 24, 1148, 2], [1, 7, 24, 348, 88, 4, 208, 82, 15, 727, 261, 16, 3, 2]]\n",
      "8923\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "num_words = len(vocab)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, filters='', lower=True, split=' ', \n",
    "                      char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "assert num_words == len(tokenizer.word_index)\n",
    "#word to index\n",
    "word_index = tokenizer.word_index\n",
    "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded_sentences[:5])\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open(MODELS_PATH + 'jokes_tokenizer_' + str(RUN_INDEX) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training data size =  833131\n",
      "Max seq len =  13\n",
      "(833131, 13)\n",
      "[[  1   4  73 130  72  50 466 201  12 288   8 118   5]\n",
      " [  4  73 130  72  50 466 201  12 288   8 118   5 201]]\n",
      "(833131, 1)\n",
      "[[201]\n",
      " [ 15]]\n"
     ]
    }
   ],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "for sentence in encoded_sentences:\n",
    "    l = len(sentence)\n",
    "    sliding_window_length = min(l-3, MAX_SEQUENCE_LENGTH)\n",
    "    step_size = 1\n",
    "    for i in range(0, l - sliding_window_length, step_size):\n",
    "        X_data.append(sentence[i:i+sliding_window_length])\n",
    "        y_data.append(sentence[i+sliding_window_length])\n",
    "        \n",
    "print(\"Total training data size = \", len(X_data))\n",
    "MAX_SEQ_LEN = max([len(seq) for seq in X_data])\n",
    "print(\"Max seq len = \", MAX_SEQ_LEN)\n",
    "X_data = pad_sequences(X_data, maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "y_data = np.array(y_data).reshape(-1,1)\n",
    "print(X_data.shape)\n",
    "print(X_data[:2])\n",
    "print(y_data.shape)\n",
    "print(y_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "#Glove Vectors\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 255\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word,i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def BiLSTM(vocab_size, embedding_dim, hidden_dim, deeper_dim, max_seq_len, \n",
    "           dropout_factor=0.5, regularization=0.00001):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,embedding_dim,input_length=max_seq_len,weights=[embedding_matrix],mask_zero=True,trainable=False))\n",
    "    model.add(Bidirectional(LSTM(hidden_dim, \n",
    "                                 activation='tanh',\n",
    "                                 kernel_regularizer=regularizers.l2(regularization),\n",
    "                                 recurrent_regularizer=regularizers.l2(regularization), unroll=True, return_sequences = True\n",
    "                                )))\n",
    "    model.add(Dropout(dropout_factor))\n",
    "    model.add(Attention(MAX_SEQUENCE_LENGTH))\n",
    "    model.add(Dense(units=deeper_dim, activation='elu', \n",
    "              kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(dropout_factor))\n",
    "    model.add(Dense(units=vocab_size, activation='softmax', \n",
    "              kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n",
    "                  metrics=[sparse_categorical_crossentropy, sparse_categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 13, 100)           892300    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 13, 1024)          2510848   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 13, 1024)          0         \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 1024)              1037      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8923)              2293211   \n",
      "=================================================================\n",
      "Total params: 5,959,796\n",
      "Trainable params: 5,067,496\n",
      "Non-trainable params: 892,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, deeper_dim=DEEPER_DIM,\n",
    "              max_seq_len=MAX_SEQ_LEN, dropout_factor=DROPOUT_FACTOR, regularization=REGULARIZATION)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TB(TensorBoard):\n",
    "    def __init__(self, log_every=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.log_every = log_every\n",
    "        self.counter = 0\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.counter+=1\n",
    "        if self.counter%self.log_every==0:\n",
    "            for name, value in logs.items():\n",
    "                if name in ['batch', 'size']:\n",
    "                    continue\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.writer.add_summary(summary, self.counter)\n",
    "            self.writer.flush()\n",
    "        \n",
    "        super().on_batch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666504 samples, validate on 166627 samples\n",
      "Epoch 1/1\n",
      "666504/666504 [==============================] - 169s 253us/step - loss: 5.7893 - sparse_categorical_crossentropy: 5.7633 - sparse_categorical_accuracy: 0.1503 - val_loss: 5.2757 - val_sparse_categorical_crossentropy: 5.2456 - val_sparse_categorical_accuracy: 0.1944\n",
      "\n",
      "Epoch 00001: saving model to ./models/checkpoints/jokes_bilstm_gen4.01-5.28.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.27568, saving model to ./models/jokes_bilstm_gen4.hdf5\n",
      "Total elapsed time:  179.44890713691711\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tensorboard = TB(log_dir=\"./logs/jokes_bilstm/{}\".format(time()), \n",
    "                          histogram_freq=0, write_graph=True, write_images=False, log_every=10)\n",
    "\n",
    "callbacks=[tensorboard, \n",
    "           EarlyStopping(patience=5, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=MODELS_PATH + 'checkpoints/jokes_bilstm_gen'+str(RUN_INDEX)+'.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=1), \n",
    "           ModelCheckpoint(filepath=MODELS_PATH + 'jokes_bilstm_gen'+str(RUN_INDEX)+'.hdf5', \n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=1, save_best_only=True)]\n",
    "\n",
    "model.fit(X_data, y_data, epochs=1, batch_size=2048, shuffle=True, verbose=1, validation_split=0.2, \n",
    "          callbacks=callbacks)\n",
    "\n",
    "print(\"Total elapsed time: \", time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate(model, tokenizer, seed_text, maxlen):\n",
    "    \n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    print(seq)\n",
    "    \n",
    "    while True:\n",
    "        if len(seq) > MAX_SEQ_LEN:\n",
    "            encoded_seq = seq[-1*MAX_SEQ_LEN:]\n",
    "        else:\n",
    "            encoded_seq = seq\n",
    "        padded_seq = pad_sequences([encoded_seq], maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "        #padded_seq = np.array([seq])\n",
    "        y_prob = model.predict(padded_seq)\n",
    "        y_class = y_prob.argmax(axis=-1)[0]\n",
    "        if y_class == 0:\n",
    "            break\n",
    "        out_word = reverse_word_map[y_class]\n",
    "        seq.append(y_class)\n",
    "        if out_word == 'eos' or len(seq) > maxlen:\n",
    "            break\n",
    "    \n",
    "    words = [reverse_word_map[idx] for idx in seq]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 7, 84, 8, 258, 5259, 12, 519]\n",
      "sos i had to use napkin and apple . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos i had to use napkin and apple\", maxlen=40)\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sos hello', 'hello ,', \", i'm\", \"i'm a\", 'a dinosaur', 'dinosaur .', '. eos']\n"
     ]
    }
   ],
   "source": [
    "def bigrams_list(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    bigrams = []\n",
    "    for i in range(0, len(words)-1):\n",
    "        bigrams.append(words[i]+' '+words[i+1])\n",
    "    return bigrams\n",
    "\n",
    "print(bigrams_list(\"sos hello , i'm a dinosaur . eos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sos a', 'a man', 'man walks', 'walks into', 'into an', 'an eye', 'eye doctor', 'doctor and', 'and asks', 'asks to', 'to see', 'see the', 'the doctor', 'doctor threedots', 'threedots the', 'the nurse', 'nurse replies', 'replies ,', ', \"', '\" not', 'not with', 'with that', 'that eye', 'eye !', '! \"', '\" eos'], ['sos last', 'last night', 'night i', 'i got', 'got a', 'a handjob', 'handjob from', 'from a', 'a blind', 'blind girl', 'girl she', 'she said', 'said ,', ', \"', '\" you\\'ve', \"you've got\", 'got the', 'the biggest', 'biggest dick', \"dick i've\", \"i've ever\", 'ever put', 'put my', 'my hands', 'hands on', 'on .', '. \"', '\" i', 'i said', 'said ,', ', \"', '\" nah', 'nah ,', \", you're\", \"you're just\", 'just pulling', 'pulling my', 'my leg', 'leg .', '. \"', '\" eos']]\n"
     ]
    }
   ],
   "source": [
    "sentence_bigrams = [bigrams_list(s) for s in sentences]\n",
    "print(sentence_bigrams[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    temp = set(lst2)\n",
    "    lst3 = [value for value in lst1 if value in temp]\n",
    "    return lst3\n",
    "\n",
    "def similarity_score(lst1, lst2):\n",
    "    intersection_len = len(intersection(lst1, lst2))\n",
    "    return (1.0*intersection_len)/len(lst1)#+len(lst2)-intersection_len)\n",
    " \n",
    "def print_closest_sentences(sentence, sentence_bigrams, top_k=3):\n",
    "    bigrams = bigrams_list(sentence)\n",
    "    scores = np.array([similarity_score(bigrams, sbigrams)\n",
    "                       for sbigrams in sentence_bigrams])\n",
    "    top_k_indices = scores.argsort()[-1*top_k:][::-1]\n",
    "    top_k_scores = scores[top_k_indices]\n",
    "    for k in range(top_k):\n",
    "        print(top_k_scores[k], \" -> \", sentences[top_k_indices[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45454545454545453  ->  sos i was getting a massage and i asked the masseuse if it was normal for a man to get an erection he replied that it was . so i asked if he could get it out of my face . eos\n",
      "0.45454545454545453  ->  sos i told my boyfriend that i felt like i had been forgetting a lot of things lately . he said , \" because i've been fucking your brains out . \" i've never laughed so hard . eos\n",
      "0.45454545454545453  ->  sos i had to take the batteries out of my carbon monoxide detector . all the beeping was giving me a headache and making me sleepy . eos\n",
      "0.45454545454545453  ->  sos i had to use my glasses when playing tennis . because its a no contact sport . eos\n",
      "0.45454545454545453  ->  sos i recently came into a lot of money . the bank teller wasn't happy about having to use gloves . eos\n",
      "0.45454545454545453  ->  sos i stepped on an ant hill today and realized i had probably killed a lot of innocent ants . i also killed all the ant rapists so , i'm a hero . eos\n",
      "0.36363636363636365  ->  sos i asked a friend of mine what it was like being a herb farmer threedots threedots he said its not so bad and that he had a lot of thyme on his hands . eos\n",
      "0.36363636363636365  ->  sos i know a lot of women who should substitute their lipstick with glue sticks . eos\n",
      "0.36363636363636365  ->  sos i had to chase a mugger after he stole my wallet . he gave me a run for my money . eos\n",
      "0.36363636363636365  ->  sos i save a lot of money on makeup by just being attractive . eos\n"
     ]
    }
   ],
   "source": [
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 =  load_model('models/checkpoints/jokes_bilstm_gen2.08-4.39.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 107, 1837]\n",
      "sos a guy finds up . period is it . eos\n",
      "0.4  ->  sos what did the baby milk say to his older sister ? you're spoiled ! eos\n",
      "0.4  ->  sos girl are you a dishwasher ? because i would like to fill you with my dirty load in the evening , turn you on , and fall asleep before you finish eos\n",
      "0.4  ->  sos if a shark attacks you , do not punch him in the nose . be the bigger person and just ignore him . eos\n",
      "0.3  ->  sos when you have the opportunity to become a bigger person , take it because cake is delicious . eos\n",
      "0.3  ->  sos what's the difference between hitler and michael phelps ? michael phelps could finish a race . eos\n",
      "0.3  ->  sos the difference between \" like \" \" love \" and \" in love \" is the same as the difference between \" for now \" \" for a while \" and \" forever \" eos\n",
      "0.3  ->  sos what do engineers use for birth control ? personality . eos\n",
      "0.3  ->  sos what's the only thing working out at the gym ? the business plan . eos\n",
      "0.3  ->  sos my wife thinks i'm cheating on her with our babysitter threedots i think she's just bitter because she's never been able to have kids threedots eos\n",
      "0.3  ->  sos knock knock who's there ? to to who ? to whom . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos a guy finds\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
