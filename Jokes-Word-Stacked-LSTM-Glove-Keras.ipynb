{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import importlib\n",
    "from library import data_preprocess as dp\n",
    "importlib.reload(dp)\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Input\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Embedding, TimeDistributed, Flatten, Merge, Concatenate\n",
    "from keras import regularizers\n",
    "from keras.metrics import sparse_categorical_accuracy, sparse_categorical_crossentropy\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://nlp.stanford.edu/data/glove.6B.zip\n",
    "DATA_PATH = './datasets/jokes.pickle'\n",
    "VOCAB_PATH = './datasets/jokes_vocabulary.pickle'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "GLOVE_PATH = './data/glove.6B.200d.txt'\n",
    "\n",
    "MODEL_PREFIX = 'jokes_stacked_lstm_glove'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 13\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "GLOVE_EMBEDDING_DIM = 200\n",
    "EMBEDDING_DIM1 = 256\n",
    "EMBEDDING_DIM2 = 256\n",
    "HIDDEN_DIM1 = 512\n",
    "HIDDEN_DIM2 = 256\n",
    "DEEPER_DIM = 256\n",
    "DROPOUT_FACTOR = 0.2\n",
    "REGULARIZATION = 0.00001\n",
    "LEARNING_RATE = 0.003\n",
    "\n",
    "DATA_PERCENT = 0.1\n",
    "\n",
    "RUN_INDEX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences =  96910\n",
      "['sos two polish guys are hunting for bears in the woods . they come across a fork in the road with a sign that says \" bear left \" they both turn around and go home . eos', 'sos i just read a book about stockholm syndrome . it was pretty bad at first , but by the end i kinda liked it . eos']\n",
      "Vocab size =  8922\n",
      "['sos', 'did', 'you', 'hear', 'about', 'the', 'new', 'corduroy', 'pillows', '?']\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as pickleFile:\n",
    "    sentences = pickle.load(pickleFile)\n",
    "\n",
    "with open(VOCAB_PATH, 'rb') as pickleFile:\n",
    "    vocab = pickle.load(pickleFile)\n",
    "    \n",
    "random.shuffle(sentences)\n",
    "\n",
    "print(\"Number of sentences = \", len(sentences))\n",
    "print(sentences[:2])\n",
    "print(\"Vocab size = \", len(vocab))\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 85, 1701, 211, 39, 4019, 30, 1579, 14, 5, 1181, 3, 37, 170, 945, 4, 2667, 14, 5, 255, 29, 4, 631, 28, 105, 11, 515, 226, 11, 37, 192, 376, 224, 12, 86, 227, 3, 2], [1, 7, 44, 394, 4, 382, 51, 6156, 2434, 3, 16, 24, 324, 178, 49, 129, 10, 41, 122, 5, 337, 7, 1715, 1245, 16, 3, 2], [1, 217, 23, 17, 36, 5, 298, 372, 60, 93, 70, 24, 368, 152, 29, 61, 142, 691, 6, 4, 23, 11, 282, 9, 47, 283, 517, 3, 11, 2], [1, 4, 73, 130, 72, 4, 97, 12, 699, 5, 315, 10, 11, 48, 22, 4, 464, 46, 3, 11, 5, 315, 700, 103, 13, 141, 3, 2], [1, 5, 129, 90, 7, 2366, 49, 13, 1880, 81, 212, 10, 61, 187, 458, 254, 362, 360, 485, 3, 1511, 10, 179, 272, 1209, 3, 2]]\n",
      "8923\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "num_words = len(vocab)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, filters='', lower=True, split=' ', \n",
    "                      char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "assert num_words == len(tokenizer.word_index)\n",
    "\n",
    "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded_sentences[:5])\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open(MODELS_PATH + MODEL_PREFIX + '_tokenizer_' + str(RUN_INDEX) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training data size =  833131\n",
      "Max seq len =  13\n",
      "(833131, 13)\n",
      "[[   1   85 1701  211   39 4019   30 1579   14    5 1181    3   37]\n",
      " [  85 1701  211   39 4019   30 1579   14    5 1181    3   37  170]]\n",
      "(833131, 13, 1)\n",
      "[[[  85]\n",
      "  [1701]\n",
      "  [ 211]\n",
      "  [  39]\n",
      "  [4019]\n",
      "  [  30]\n",
      "  [1579]\n",
      "  [  14]\n",
      "  [   5]\n",
      "  [1181]\n",
      "  [   3]\n",
      "  [  37]\n",
      "  [ 170]]\n",
      "\n",
      " [[1701]\n",
      "  [ 211]\n",
      "  [  39]\n",
      "  [4019]\n",
      "  [  30]\n",
      "  [1579]\n",
      "  [  14]\n",
      "  [   5]\n",
      "  [1181]\n",
      "  [   3]\n",
      "  [  37]\n",
      "  [ 170]\n",
      "  [ 945]]]\n"
     ]
    }
   ],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "for sentence in encoded_sentences:\n",
    "    l = len(sentence)\n",
    "    sliding_window_length = min(l-3, MAX_SEQUENCE_LENGTH)\n",
    "    step_size = 1\n",
    "    for i in range(0, l - sliding_window_length, step_size):\n",
    "        X_data.append(sentence[i:i+sliding_window_length])\n",
    "        y_data.append(sentence[i+1:i+sliding_window_length+1])\n",
    "        \n",
    "print(\"Total training data size = \", len(X_data))\n",
    "MAX_SEQ_LEN = max([len(seq) for seq in X_data])\n",
    "print(\"Max seq len = \", MAX_SEQ_LEN)\n",
    "X_data = pad_sequences(X_data, maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "y_data = pad_sequences(y_data, maxlen=MAX_SEQ_LEN, padding='pre').reshape(-1, MAX_SEQ_LEN, 1)\n",
    "#y_data = np.array(y_data).reshape(-1,1)\n",
    "print(X_data.shape)\n",
    "print(X_data[:2])\n",
    "print(y_data.shape)\n",
    "print(y_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing glove word vectors\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing glove word vectors')\n",
    "#Glove Vectors\n",
    "glove_embeddings_index = {}\n",
    "f = open(GLOVE_PATH)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors.' % len(glove_embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing glove embedding matrix\n",
      "Null word embeddings: 255\n",
      "(8923, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Preparing glove embedding matrix')\n",
    "glove_embedding_matrix = np.zeros((VOCAB_SIZE, GLOVE_EMBEDDING_DIM))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    embedding_vector = glove_embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        glove_embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))\n",
    "print(glove_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def StackedLSTM(vocab_size, glove_embedding_dim, glove_embedding_matrix, embedding_dim1, embedding_dim2,\n",
    "           hidden_dim1, hidden_dim2, deeper_dim, max_seq_len, \n",
    "           dropout_factor=0.5, regularization=0.00001, learning_rate=0.001):\n",
    "    \n",
    "    inputs = Input(shape=(None,))\n",
    "    \n",
    "    glove_embedding = Embedding(vocab_size, glove_embedding_dim, #input_length=max_seq_len,\n",
    "                                  weights=[glove_embedding_matrix],\n",
    "                                  mask_zero=True,trainable=False)(inputs)\n",
    "    \n",
    "    \n",
    "    word_embedding = Embedding(vocab_size, embedding_dim1, #input_length=max_seq_len, \n",
    "                               mask_zero=True, embeddings_regularizer=regularizers.l2(regularization))(inputs)\n",
    "    \n",
    "    concat_embeds = Concatenate(axis=-1)([glove_embedding, word_embedding])\n",
    "    \n",
    "    final_embed = Dense(units=embedding_dim2, activation='tanh',\n",
    "                        kernel_regularizer=regularizers.l2(regularization))(concat_embeds)\n",
    "    \n",
    "    lstm1 = LSTM(hidden_dim1, activation='tanh', \n",
    "                   kernel_regularizer=regularizers.l2(regularization), \n",
    "                   recurrent_regularizer=regularizers.l2(regularization), #unroll=True, \n",
    "                   return_sequences = True, dropout=dropout_factor, recurrent_dropout=dropout_factor)(final_embed)\n",
    "    \n",
    "    lstm2 = LSTM(hidden_dim2, activation='tanh', \n",
    "                   kernel_regularizer=regularizers.l2(regularization), \n",
    "                   recurrent_regularizer=regularizers.l2(regularization), #unroll=True, \n",
    "                   return_sequences = True, dropout=dropout_factor, recurrent_dropout=dropout_factor)(lstm1)\n",
    "    \n",
    "    timedist_dropout = TimeDistributed(Dropout(dropout_factor))(lstm2)\n",
    "    \n",
    "    deep_dense = Dense(units=deeper_dim, activation='tanh', \n",
    "                       kernel_regularizer=regularizers.l2(regularization))(timedist_dropout)\n",
    "    \n",
    "    dropout_layer1 = Dropout(dropout_factor)(deep_dense)\n",
    "    \n",
    "    outputs = Dense(units=vocab_size, activation='softmax', \n",
    "                    kernel_regularizer=regularizers.l2(regularization))(dropout_layer1)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=learning_rate),\n",
    "                  metrics=[sparse_categorical_crossentropy, sparse_categorical_accuracy], sample_weight_mode='temporal')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    1784600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    2284288     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 456)    0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 256)    116992      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, None, 512)    1574912     dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, None, 256)    787456      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 256)    0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 256)    65792       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 256)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 8923)   2293211     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,907,251\n",
      "Trainable params: 7,122,651\n",
      "Non-trainable params: 1,784,600\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "model = StackedLSTM(vocab_size=VOCAB_SIZE, glove_embedding_dim=GLOVE_EMBEDDING_DIM,\n",
    "                    glove_embedding_matrix=glove_embedding_matrix, \n",
    "                    embedding_dim1=EMBEDDING_DIM1, embedding_dim2=EMBEDDING_DIM2,\n",
    "                    hidden_dim1=HIDDEN_DIM1, hidden_dim2=HIDDEN_DIM2,\n",
    "                    deeper_dim=DEEPER_DIM, max_seq_len=MAX_SEQ_LEN, dropout_factor=DROPOUT_FACTOR, \n",
    "                    regularization=REGULARIZATION, learning_rate=LEARNING_RATE)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TB(TensorBoard):\n",
    "    def __init__(self, log_every=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.log_every = log_every\n",
    "        self.counter = 0\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.counter+=1\n",
    "        if self.counter%self.log_every==0:\n",
    "            for name, value in logs.items():\n",
    "                if name in ['batch', 'size']:\n",
    "                    continue\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.writer.add_summary(summary, self.counter)\n",
    "            self.writer.flush()\n",
    "        \n",
    "        super().on_batch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666504 samples, validate on 166627 samples\n",
      "Epoch 1/20\n",
      "666504/666504 [==============================] - 473s 710us/step - loss: 6.4779 - sparse_categorical_crossentropy: 6.4545 - sparse_categorical_accuracy: 0.0370 - val_loss: 6.2857 - val_sparse_categorical_crossentropy: 6.2554 - val_sparse_categorical_accuracy: 0.0396\n",
      "\n",
      "Epoch 00001: saving model to ./models/checkpoints/jokes_stacked_lstm_glove_gen1.01-6.29.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.28574, saving model to ./models/jokes_stacked_lstm_glove_gen1.hdf5\n",
      "Epoch 2/20\n",
      "666504/666504 [==============================] - 476s 714us/step - loss: 5.4573 - sparse_categorical_crossentropy: 5.3864 - sparse_categorical_accuracy: 0.1266 - val_loss: 4.8011 - val_sparse_categorical_crossentropy: 4.6716 - val_sparse_categorical_accuracy: 0.2131\n",
      "\n",
      "Epoch 00002: saving model to ./models/checkpoints/jokes_stacked_lstm_glove_gen1.02-4.80.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.28574 to 4.80107, saving model to ./models/jokes_stacked_lstm_glove_gen1.hdf5\n",
      "Epoch 3/20\n",
      "666504/666504 [==============================] - 478s 716us/step - loss: 4.5971 - sparse_categorical_crossentropy: 4.4199 - sparse_categorical_accuracy: 0.2330 - val_loss: 4.4869 - val_sparse_categorical_crossentropy: 4.2713 - val_sparse_categorical_accuracy: 0.2590\n",
      "\n",
      "Epoch 00003: saving model to ./models/checkpoints/jokes_stacked_lstm_glove_gen1.03-4.49.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.80107 to 4.48693, saving model to ./models/jokes_stacked_lstm_glove_gen1.hdf5\n",
      "Epoch 4/20\n",
      "666504/666504 [==============================] - 477s 716us/step - loss: 4.3850 - sparse_categorical_crossentropy: 4.1427 - sparse_categorical_accuracy: 0.2631 - val_loss: 4.3973 - val_sparse_categorical_crossentropy: 4.1346 - val_sparse_categorical_accuracy: 0.2771\n",
      "\n",
      "Epoch 00004: saving model to ./models/checkpoints/jokes_stacked_lstm_glove_gen1.04-4.40.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.48693 to 4.39733, saving model to ./models/jokes_stacked_lstm_glove_gen1.hdf5\n",
      "Epoch 5/20\n",
      "666504/666504 [==============================] - 477s 715us/step - loss: 4.2900 - sparse_categorical_crossentropy: 4.0102 - sparse_categorical_accuracy: 0.2780 - val_loss: 4.3565 - val_sparse_categorical_crossentropy: 4.0645 - val_sparse_categorical_accuracy: 0.2873\n",
      "\n",
      "Epoch 00005: saving model to ./models/checkpoints/jokes_stacked_lstm_glove_gen1.05-4.36.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.39733 to 4.35649, saving model to ./models/jokes_stacked_lstm_glove_gen1.hdf5\n",
      "Epoch 6/20\n",
      "666504/666504 [==============================] - 477s 716us/step - loss: 4.2345 - sparse_categorical_crossentropy: 3.9304 - sparse_categorical_accuracy: 0.2871 - val_loss: 4.3338 - val_sparse_categorical_crossentropy: 4.0215 - val_sparse_categorical_accuracy: 0.2930\n",
      "\n",
      "Epoch 00006: saving model to ./models/checkpoints/jokes_stacked_lstm_glove_gen1.06-4.33.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.35649 to 4.33377, saving model to ./models/jokes_stacked_lstm_glove_gen1.hdf5\n",
      "Epoch 7/20\n",
      "666504/666504 [==============================] - 476s 715us/step - loss: 4.1954 - sparse_categorical_crossentropy: 3.8739 - sparse_categorical_accuracy: 0.2934 - val_loss: 4.3173 - val_sparse_categorical_crossentropy: 3.9903 - val_sparse_categorical_accuracy: 0.2974\n",
      "\n",
      "Epoch 00007: saving model to ./models/checkpoints/jokes_stacked_lstm_glove_gen1.07-4.32.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.33377 to 4.31729, saving model to ./models/jokes_stacked_lstm_glove_gen1.hdf5\n",
      "Epoch 8/20\n",
      "666504/666504 [==============================] - 477s 716us/step - loss: 4.1655 - sparse_categorical_crossentropy: 3.8309 - sparse_categorical_accuracy: 0.2985 - val_loss: 4.3087 - val_sparse_categorical_crossentropy: 3.9701 - val_sparse_categorical_accuracy: 0.3006\n",
      "\n",
      "Epoch 00008: saving model to ./models/checkpoints/jokes_stacked_lstm_glove_gen1.08-4.31.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.31729 to 4.30865, saving model to ./models/jokes_stacked_lstm_glove_gen1.hdf5\n",
      "Epoch 9/20\n",
      "666504/666504 [==============================] - 477s 716us/step - loss: 4.1421 - sparse_categorical_crossentropy: 3.7971 - sparse_categorical_accuracy: 0.3024 - val_loss: 4.3016 - val_sparse_categorical_crossentropy: 3.9534 - val_sparse_categorical_accuracy: 0.3030\n",
      "\n",
      "Epoch 00009: saving model to ./models/checkpoints/jokes_stacked_lstm_glove_gen1.09-4.30.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.30865 to 4.30156, saving model to ./models/jokes_stacked_lstm_glove_gen1.hdf5\n",
      "Epoch 10/20\n",
      " 18432/666504 [..............................] - ETA: 7:11 - loss: 4.1145 - sparse_categorical_crossentropy: 3.7660 - sparse_categorical_accuracy: 0.3046"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-6770a15a26cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                            monitor='val_loss', verbose=1, mode='auto', period=1, save_best_only=True)]\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total elapsed time: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tensorboard = TB(log_dir=\"./logs/\" + MODEL_PREFIX + \"/{}\".format(time()), \n",
    "                          histogram_freq=0, write_graph=True, write_images=False, log_every=10)\n",
    "\n",
    "callbacks=[tensorboard, \n",
    "           EarlyStopping(patience=5, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=MODELS_PATH + 'checkpoints/'+ MODEL_PREFIX + '_gen' + str(RUN_INDEX) + '.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=1), \n",
    "           ModelCheckpoint(filepath=MODELS_PATH + MODEL_PREFIX + '_gen'+str(RUN_INDEX)+'.hdf5', \n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=1, save_best_only=True)]\n",
    "\n",
    "model.fit(X_data, y_data, epochs=20, batch_size=2048, shuffle=True, verbose=1, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "print(\"Total elapsed time: \", time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate(model, tokenizer, seed_text, maxlen, probabilistic=False, exploration_factor=1.0):\n",
    "    \n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    print(seq)\n",
    "    \n",
    "    while True:\n",
    "        encoded_seq = seq\n",
    "        if len(seq) > MAX_SEQ_LEN:\n",
    "            encoded_seq = encoded_seq[-1*MAX_SEQ_LEN:]\n",
    "            \n",
    "        #padded_seq = pad_sequences([encoded_seq], maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "        padded_seq = np.array([seq])\n",
    "        y_prob = model.predict(padded_seq)[0][-1].reshape(1,-1)#[3:].reshape(-1,1)\n",
    "        \n",
    "        if random.random() <= exploration_factor:\n",
    "            probabilistic = True\n",
    "        else:\n",
    "            probabilistic = False\n",
    "            \n",
    "        if probabilistic:\n",
    "            y_class = np.argmax(np.random.multinomial(1,y_prob[0]/(np.sum(y_prob[0])+1e-5),1))\n",
    "        else:\n",
    "            y_class = y_prob.argmax(axis=-1)[0]\n",
    "        \n",
    "        if y_class == 0:\n",
    "            break\n",
    "        out_word = reverse_word_map[y_class]\n",
    "        seq.append(y_class)\n",
    "        if out_word == 'eos' or len(seq) > maxlen or out_word == 'sos':\n",
    "            break\n",
    "    \n",
    "    words = [reverse_word_map[idx] for idx in seq]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "model = load_model('models/jokes_stacked_lstm_glove_gen1.hdf5')\n",
    "with open('models/jokes_stacked_lstm_glove_tokenizer_1.pickle', 'rb') as pickleFile:\n",
    "    tokenizer = pickle.load(pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 13, 108, 720]\n",
      "sos my wife likes me for my death . i know what i want . i just don't know what she laced them with but i've been tripping all day . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos my wife likes\", maxlen=40, exploration_factor=0.3)\n",
    "print(joke)\n",
    "#sos a little boy walks into a bar threedots and asks , \" is the bar tender here ? \" eos\n",
    "#sos a little boy walks into a bar threedots bartender says , \" hey , what is this , some kind of joke ? \" eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sos hello', 'hello ,', \", i'm\", \"i'm a\", 'a dinosaur', 'dinosaur .', '. eos']\n"
     ]
    }
   ],
   "source": [
    "def bigrams_list(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    bigrams = []\n",
    "    for i in range(0, len(words)-1):\n",
    "        bigrams.append(words[i]+' '+words[i+1])\n",
    "    return bigrams\n",
    "\n",
    "print(bigrams_list(\"sos hello , i'm a dinosaur . eos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sos two', 'two polish', 'polish guys', 'guys are', 'are hunting', 'hunting for', 'for bears', 'bears in', 'in the', 'the woods', 'woods .', '. they', 'they come', 'come across', 'across a', 'a fork', 'fork in', 'in the', 'the road', 'road with', 'with a', 'a sign', 'sign that', 'that says', 'says \"', '\" bear', 'bear left', 'left \"', '\" they', 'they both', 'both turn', 'turn around', 'around and', 'and go', 'go home', 'home .', '. eos'], ['sos i', 'i just', 'just read', 'read a', 'a book', 'book about', 'about stockholm', 'stockholm syndrome', 'syndrome .', '. it', 'it was', 'was pretty', 'pretty bad', 'bad at', 'at first', 'first ,', ', but', 'but by', 'by the', 'the end', 'end i', 'i kinda', 'kinda liked', 'liked it', 'it .', '. eos']]\n"
     ]
    }
   ],
   "source": [
    "sentence_bigrams = [bigrams_list(s) for s in sentences]\n",
    "print(sentence_bigrams[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    temp = set(lst2)\n",
    "    lst3 = [value for value in lst1 if value in temp]\n",
    "    return lst3\n",
    "\n",
    "def similarity_score(lst1, lst2):\n",
    "    intersection_len = len(intersection(lst1, lst2))\n",
    "    return (1.0*intersection_len)/len(lst1)#+len(lst2)-intersection_len)\n",
    " \n",
    "def print_closest_sentences(sentence, sentence_bigrams, top_k=3):\n",
    "    bigrams = bigrams_list(sentence)\n",
    "    scores = np.array([similarity_score(bigrams, sbigrams)\n",
    "                       for sbigrams in sentence_bigrams])\n",
    "    top_k_indices = scores.argsort()[-1*top_k:][::-1]\n",
    "    top_k_scores = scores[top_k_indices]\n",
    "    for k in range(top_k):\n",
    "        print(top_k_scores[k], \" -> \", sentences[top_k_indices[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6896551724137931  ->  sos so a jew , a muslim , and a christian walk into a bar threedots and the bartender says , \" what is this , some kind of joke ? \" eos\n",
      "0.6896551724137931  ->  sos a blonde , a brunette , and a redhead walk into a bar threedots and the bartender says , \" what is this , some kind of joke ? \" eos\n",
      "0.6551724137931034  ->  sos so this blonde , rabbi , and priest walk in to a bar threedots threedots and the bartender says , \" what is this , some kind of joke ? \" eos\n",
      "0.6551724137931034  ->  sos a bishop , a pastor and the pope walk into a bar threedots the bartender says , \" what is this , some kind of joke ? \" eos\n",
      "0.6206896551724138  ->  sos a priest , a rabbi , and a minister walk into a bar threedots the bartender says , \" what is this , some kind of joke ? \" eos\n",
      "0.6206896551724138  ->  sos a priest , a rabbi , a horse , and a blonde walk into a bar threedots threedots the bartender says , \" what is this , some kind of joke ? \" eos\n",
      "0.5862068965517241  ->  sos an englishman , an irishman , a scotsman , a priest , a rabbi and adolf hitler walk into a bar . the bartender says , \" what is this , some kind of joke ? \" eos\n",
      "0.5862068965517241  ->  sos a nun , a priest , and a rabbi walk into a bar threedots the bartender looks up and says , \" what is this , some kind of joke ? \" eos\n",
      "0.5862068965517241  ->  sos a priest , a rabbi , a blonde and a horse all walk into a bar threedots the bartender looks up and says , \" what is this , some kind of joke ? \" eos\n",
      "0.5862068965517241  ->  sos a priest , a horse and a leprechaun walk into a bar . the bartender says , \" what is this , some kind of joke ? \" eos\n"
     ]
    }
   ],
   "source": [
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 17, 20, 9, 66]\n",
      "sos what do you call the new best pupil ? jose and hose d boo . eos\n",
      "0.375  ->  sos what do you call the only wood that doesn't float ? natalie . eos\n",
      "0.375  ->  sos what do you call the deaf man with no limbs ? whatever you want . eos\n",
      "0.375  ->  sos what do you call it when the new us president waves his hand ? a microwave . eos\n",
      "0.375  ->  sos what do you call the bad part of italy ? the spaghetto . eos\n",
      "0.375  ->  sos what do you call the men i kidnap and jerk off against their will ? stroke victims . eos\n",
      "0.375  ->  sos what do you call the bad part of italy ? spaghetto . eos\n",
      "0.375  ->  sos what do you call the useless skin on the end of a penis ? a man . courtesy of my grandmother . eos\n",
      "0.375  ->  sos what do you call the useless piece is skin on the end of the penis ? the man . eos\n",
      "0.375  ->  sos what do you call the international program that helps clowns with their personal space problems ? clowns without borders . eos\n",
      "0.375  ->  sos what do you call the child of two parents with downs syndrome ? a hand - me - down . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos what do you call\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sos what did the farmer say when he lost his tractor ? have you seen my tractor ? eos', \"sos how do find the blind man at the nudist colony ? it's not hard . eos\", 'sos only a woman can make you a millionaire if you are a billionaire . eos', 'sos i used to play water polo but i had to stop when my horse drowned eos', \"sos harry : want to see a magic trick ? voldemort : let's see what you got potter . harry : got your nose ! voldemort : you know i hate that game . eos\", \"sos what's the difference between michael phelps and hitler threedots michael phelps can finish a race eos\", 'sos * tries to get a life . life : i have a girlfriend . eos', \"sos it funny that when it's black on white , it's a crime . when it it's white on black , it's a hate crime . eos\", \"sos living in switzerland wouldn't be so bad . the flag is a plus . eos\", 'sos yo mama is so hairy that bigfoot tried to take her picture ! eos']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 5, 76, 74, 167]\n",
      "sos what's the difference between being proud , gay man hides in switzerland ? threedots it's just that spicy . eos\n",
      "0.3  ->  sos what's the difference between a gay man and a refrigerator ? my refrigerator doesn't fart when i pull my meat out of it . eos\n",
      "0.3  ->  sos what's the difference between a gay man and a freezer ? the freezer doesn't fart when you pull the meat out . eos\n",
      "0.3  ->  sos what's the difference between a gay man and a freezer ? the freezer doesn't fart when you pull your meat out . eos\n",
      "0.3  ->  sos what's the difference between being hungry and horny ? where you put the cucumber . eos\n",
      "0.3  ->  sos what's the difference between a sliced up body and a new bmw ? threedots i don't have a new bmw in my garage . eos\n",
      "0.3  ->  sos what's the difference between a terrorist cell and a children's hospital ? threedots don't ask me man , i just fly the drones . eos\n",
      "0.3  ->  sos what's the difference between a ferrari and a boner ? threedots i have a boner . eos\n",
      "0.3  ->  sos what's the difference between a fridge and a gay man ? a fridge doesn't fart when you pull the meat out . eos\n",
      "0.3  ->  sos what's the difference between politics and professional wrestling ? threedots in professional wrestling they know what they are doing . eos\n",
      "0.3  ->  sos what's the difference between a gay man and a refrigerator ? the refrigerator doesn't fart when you pull your meat out . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos what's the difference between being\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 =  load_model('models/checkpoints/jokes_bilstm_gen2.08-4.39.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 7, 237, 4, 287, 71, 720]\n",
      "sos i am a boy who likes me , there's a better place to drink nice stuff until that i sleep with a bunch of slippers when she giggles . eos\n",
      "0.16666666666666666  ->  sos i sleep with a knife under my bed in case i can't open my midnight snacks . it also comes in handy if people try to steal them . eos\n",
      "0.16666666666666666  ->  sos i think i can speak for everyone when i say that i am a ventriloquist . eos\n",
      "0.16666666666666666  ->  sos i sleep with a water gun near my bed , in case of cat burglar . eos\n",
      "0.16666666666666666  ->  sos i sleep with a squirt gun under my pillow just in case a gang of cats break in while i'm sleeping . eos\n",
      "0.16666666666666666  ->  sos i treat my body like a temple . by that i mean that a bunch of jewish guys enter me every friday night . eos\n",
      "0.16666666666666666  ->  sos a lady that sat next to me on a plane freaked out when she realized that i am a muslim . i laughed so hard my grenades nearly fell out of my pockets . eos\n",
      "0.16666666666666666  ->  sos i sleep with a knife under my pillow . you never know when someone is going to break in and give you a cake . eos\n",
      "0.16666666666666666  ->  sos i like my men how i like my rum smooth , dark , rich , and with a bunch of coke . eos\n",
      "0.16666666666666666  ->  sos i'd never lie just to get a girl to sleep with me , is one of my favorite lies to tell girls that i am trying to sleep with . eos\n",
      "0.13333333333333333  ->  sos i don't mean to brag , but instead of teaching a man how to fish , i just gave him a bunch of things to do when life hands him lemons . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos i am a boy who likes\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
