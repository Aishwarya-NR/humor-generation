{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import importlib\n",
    "from library import data_preprocess as dp\n",
    "importlib.reload(dp)\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Input\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Embedding, TimeDistributed, Flatten, Merge, Concatenate\n",
    "from keras import regularizers\n",
    "from keras.metrics import sparse_categorical_accuracy, sparse_categorical_crossentropy\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://nlp.stanford.edu/data/glove.6B.zip\n",
    "DATA_PATH = './datasets/jokes.pickle'\n",
    "VOCAB_PATH = './datasets/jokes_vocabulary.pickle'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "GLOVE_PATH = './data/glove.6B.200d.txt'\n",
    "\n",
    "MODEL_PREFIX = 'jokes_stacked_lstm_glove'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 13\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "GLOVE_EMBEDDING_DIM = 200\n",
    "EMBEDDING_DIM1 = 256\n",
    "EMBEDDING_DIM2 = 256\n",
    "HIDDEN_DIM1 = 512\n",
    "HIDDEN_DIM2 = 256\n",
    "DEEPER_DIM = 256\n",
    "DROPOUT_FACTOR = 0.2\n",
    "REGULARIZATION = 0.00001\n",
    "LEARNING_RATE = 0.003\n",
    "\n",
    "DATA_PERCENT = 0.1\n",
    "\n",
    "RUN_INDEX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences =  96910\n",
      "['sos what did the farmer say when he lost his tractor ? have you seen my tractor ? eos', \"sos how do find the blind man at the nudist colony ? it's not hard . eos\"]\n",
      "Vocab size =  8922\n",
      "['sos', 'did', 'you', 'hear', 'about', 'the', 'new', 'corduroy', 'pillows', '?']\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as pickleFile:\n",
    "    sentences = pickle.load(pickleFile)\n",
    "\n",
    "with open(VOCAB_PATH, 'rb') as pickleFile:\n",
    "    vocab = pickle.load(pickleFile)\n",
    "    \n",
    "random.shuffle(sentences)\n",
    "\n",
    "print(\"Number of sentences = \", len(sentences))\n",
    "print(sentences[:2])\n",
    "print(\"Vocab size = \", len(vocab))\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 17, 36, 5, 963, 60, 34, 31, 317, 53, 1809, 6, 27, 9, 386, 13, 1809, 6, 2], [1, 32, 20, 186, 5, 298, 73, 49, 5, 2202, 3236, 6, 57, 58, 196, 3, 2], [1, 102, 4, 138, 59, 88, 9, 4, 2747, 38, 9, 39, 4, 3335, 3, 2], [1, 7, 232, 8, 277, 291, 5950, 41, 7, 84, 8, 182, 34, 13, 372, 3701, 2], [1, 1117, 23, 96, 8, 118, 4, 1630, 1263, 6, 4928, 23, 646, 118, 17, 9, 77, 1376, 3, 1117, 23, 77, 35, 622, 21, 4928, 23, 9, 67, 7, 221, 28, 419, 3, 2]]\n",
      "8923\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "num_words = len(vocab)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, filters='', lower=True, split=' ', \n",
    "                      char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "assert num_words == len(tokenizer.word_index)\n",
    "\n",
    "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded_sentences[:5])\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open(MODELS_PATH + MODEL_PREFIX + '_tokenizer_' + str(RUN_INDEX) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training data size =  833131\n",
      "Max seq len =  13\n",
      "(833131, 13)\n",
      "[[   1   17   36    5  963   60   34   31  317   53 1809    6   27]\n",
      " [  17   36    5  963   60   34   31  317   53 1809    6   27    9]]\n",
      "(833131, 13, 1)\n",
      "[[[  17]\n",
      "  [  36]\n",
      "  [   5]\n",
      "  [ 963]\n",
      "  [  60]\n",
      "  [  34]\n",
      "  [  31]\n",
      "  [ 317]\n",
      "  [  53]\n",
      "  [1809]\n",
      "  [   6]\n",
      "  [  27]\n",
      "  [   9]]\n",
      "\n",
      " [[  36]\n",
      "  [   5]\n",
      "  [ 963]\n",
      "  [  60]\n",
      "  [  34]\n",
      "  [  31]\n",
      "  [ 317]\n",
      "  [  53]\n",
      "  [1809]\n",
      "  [   6]\n",
      "  [  27]\n",
      "  [   9]\n",
      "  [ 386]]]\n"
     ]
    }
   ],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "for sentence in encoded_sentences:\n",
    "    l = len(sentence)\n",
    "    sliding_window_length = min(l-3, MAX_SEQUENCE_LENGTH)\n",
    "    step_size = 1\n",
    "    for i in range(0, l - sliding_window_length, step_size):\n",
    "        X_data.append(sentence[i:i+sliding_window_length])\n",
    "        y_data.append(sentence[i+1:i+sliding_window_length+1])\n",
    "        \n",
    "print(\"Total training data size = \", len(X_data))\n",
    "MAX_SEQ_LEN = max([len(seq) for seq in X_data])\n",
    "print(\"Max seq len = \", MAX_SEQ_LEN)\n",
    "X_data = pad_sequences(X_data, maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "y_data = pad_sequences(y_data, maxlen=MAX_SEQ_LEN, padding='pre').reshape(-1, MAX_SEQ_LEN, 1)\n",
    "#y_data = np.array(y_data).reshape(-1,1)\n",
    "print(X_data.shape)\n",
    "print(X_data[:2])\n",
    "print(y_data.shape)\n",
    "print(y_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing glove word vectors\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing glove word vectors')\n",
    "#Glove Vectors\n",
    "glove_embeddings_index = {}\n",
    "f = open(GLOVE_PATH)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors.' % len(glove_embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing glove embedding matrix\n",
      "Null word embeddings: 255\n",
      "(8923, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Preparing glove embedding matrix')\n",
    "glove_embedding_matrix = np.zeros((VOCAB_SIZE, GLOVE_EMBEDDING_DIM))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    embedding_vector = glove_embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        glove_embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))\n",
    "print(glove_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def StackedLSTM(vocab_size, glove_embedding_dim, glove_embedding_matrix, embedding_dim1, embedding_dim2,\n",
    "           hidden_dim1, hidden_dim2, deeper_dim, max_seq_len, \n",
    "           dropout_factor=0.5, regularization=0.00001, learning_rate=0.001):\n",
    "    \n",
    "    inputs = Input(shape=(None,))\n",
    "    \n",
    "    glove_embedding = Embedding(vocab_size, glove_embedding_dim, #input_length=max_seq_len,\n",
    "                                  weights=[glove_embedding_matrix],\n",
    "                                  mask_zero=True,trainable=False)(inputs)\n",
    "    \n",
    "    \n",
    "    word_embedding = Embedding(vocab_size, embedding_dim1, #input_length=max_seq_len, \n",
    "                               mask_zero=True, embeddings_regularizer=regularizers.l2(regularization))(inputs)\n",
    "    \n",
    "    concat_embeds = Concatenate(axis=-1)([glove_embedding, word_embedding])\n",
    "    \n",
    "    final_embed = Dense(units=embedding_dim2, activation='tanh',\n",
    "                        kernel_regularizer=regularizers.l2(regularization))(concat_embeds)\n",
    "    \n",
    "    lstm1 = LSTM(hidden_dim1, activation='tanh', \n",
    "                   kernel_regularizer=regularizers.l2(regularization), \n",
    "                   recurrent_regularizer=regularizers.l2(regularization), #unroll=True, \n",
    "                   return_sequences = True, dropout=dropout_factor, recurrent_dropout=dropout_factor)(final_embed)\n",
    "    \n",
    "    lstm2 = LSTM(hidden_dim2, activation='tanh', \n",
    "                   kernel_regularizer=regularizers.l2(regularization), \n",
    "                   recurrent_regularizer=regularizers.l2(regularization), #unroll=True, \n",
    "                   return_sequences = True, dropout=dropout_factor, recurrent_dropout=dropout_factor)(lstm1)\n",
    "    \n",
    "    timedist_dropout = TimeDistributed(Dropout(dropout_factor))(lstm2)\n",
    "    \n",
    "    deep_dense = Dense(units=deeper_dim, activation='tanh', \n",
    "                       kernel_regularizer=regularizers.l2(regularization))(timedist_dropout)\n",
    "    \n",
    "    dropout_layer1 = Dropout(dropout_factor)(deep_dense)\n",
    "    \n",
    "    outputs = Dense(units=vocab_size, activation='softmax', \n",
    "                    kernel_regularizer=regularizers.l2(regularization))(dropout_layer1)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=learning_rate),\n",
    "                  metrics=[sparse_categorical_crossentropy, sparse_categorical_accuracy], sample_weight_mode='temporal')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    1784600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    2284288     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 456)    0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 256)    116992      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, None, 512)    1574912     dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, None, 256)    787456      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 256)    0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 256)    65792       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 256)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 8923)   2293211     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,907,251\n",
      "Trainable params: 7,122,651\n",
      "Non-trainable params: 1,784,600\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "model = StackedLSTM(vocab_size=VOCAB_SIZE, glove_embedding_dim=GLOVE_EMBEDDING_DIM,\n",
    "                    glove_embedding_matrix=glove_embedding_matrix, \n",
    "                    embedding_dim1=EMBEDDING_DIM1, embedding_dim2=EMBEDDING_DIM2,\n",
    "                    hidden_dim1=HIDDEN_DIM1, hidden_dim2=HIDDEN_DIM2,\n",
    "                    deeper_dim=DEEPER_DIM, max_seq_len=MAX_SEQ_LEN, dropout_factor=DROPOUT_FACTOR, \n",
    "                    regularization=REGULARIZATION, learning_rate=LEARNING_RATE)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TB(TensorBoard):\n",
    "    def __init__(self, log_every=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.log_every = log_every\n",
    "        self.counter = 0\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.counter+=1\n",
    "        if self.counter%self.log_every==0:\n",
    "            for name, value in logs.items():\n",
    "                if name in ['batch', 'size']:\n",
    "                    continue\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.writer.add_summary(summary, self.counter)\n",
    "            self.writer.flush()\n",
    "        \n",
    "        super().on_batch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666504 samples, validate on 166627 samples\n",
      "Epoch 1/20\n",
      "666504/666504 [==============================] - 473s 710us/step - loss: 6.4779 - sparse_categorical_crossentropy: 6.4545 - sparse_categorical_accuracy: 0.0370 - val_loss: 6.2857 - val_sparse_categorical_crossentropy: 6.2554 - val_sparse_categorical_accuracy: 0.0396\n",
      "\n",
      "Epoch 00001: saving model to ./models/checkpoints/jokes_stacked_lstm_glove_gen1.01-6.29.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.28574, saving model to ./models/jokes_stacked_lstm_glove_gen1.hdf5\n",
      "Epoch 2/20\n",
      "666504/666504 [==============================] - 476s 714us/step - loss: 5.4573 - sparse_categorical_crossentropy: 5.3864 - sparse_categorical_accuracy: 0.1266 - val_loss: 4.8011 - val_sparse_categorical_crossentropy: 4.6716 - val_sparse_categorical_accuracy: 0.2131\n",
      "\n",
      "Epoch 00002: saving model to ./models/checkpoints/jokes_stacked_lstm_glove_gen1.02-4.80.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.28574 to 4.80107, saving model to ./models/jokes_stacked_lstm_glove_gen1.hdf5\n",
      "Epoch 3/20\n",
      " 10240/666504 [..............................] - ETA: 7:10 - loss: 4.8039 - sparse_categorical_crossentropy: 4.6737 - sparse_categorical_accuracy: 0.2064"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tensorboard = TB(log_dir=\"./logs/\" + MODEL_PREFIX + \"/{}\".format(time()), \n",
    "                          histogram_freq=0, write_graph=True, write_images=False, log_every=10)\n",
    "\n",
    "callbacks=[tensorboard, \n",
    "           EarlyStopping(patience=5, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=MODELS_PATH + 'checkpoints/'+ MODEL_PREFIX + '_gen' + str(RUN_INDEX) + '.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=1), \n",
    "           ModelCheckpoint(filepath=MODELS_PATH + MODEL_PREFIX + '_gen'+str(RUN_INDEX)+'.hdf5', \n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=1, save_best_only=True)]\n",
    "\n",
    "model.fit(X_data, y_data, epochs=20, batch_size=2048, shuffle=True, verbose=1, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "print(\"Total elapsed time: \", time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate(model, tokenizer, seed_text, maxlen):\n",
    "    \n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    print(seq)\n",
    "    \n",
    "    while True:\n",
    "        if len(seq) > MAX_SEQ_LEN:\n",
    "            encoded_seq = seq[-1*MAX_SEQ_LEN:]\n",
    "        else:\n",
    "            encoded_seq = seq\n",
    "        padded_seq = pad_sequences([encoded_seq], maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "        #padded_seq = np.array([seq])\n",
    "        y_prob = model.predict(padded_seq)[0][-1].reshape(1,-1)#[3:].reshape(-1,1)\n",
    "        #print(y_prob.shape)\n",
    "        #y_class = y_prob.argmax(axis=-1)[0]\n",
    "        y_class = np.argmax(np.random.multinomial(1,y_prob.squeeze(axis=0),1))\n",
    "        #print(y_prob)\n",
    "        #print(y_class)\n",
    "        if y_class == 0:\n",
    "            break\n",
    "        out_word = reverse_word_map[y_class]\n",
    "        seq.append(y_class)\n",
    "        if out_word == 'eos' or len(seq) > maxlen:\n",
    "            break\n",
    "    \n",
    "    words = [reverse_word_map[idx] for idx in seq]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 7, 84, 8, 258]\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "sum(pvals[:-1]) > 1.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6df94c8249b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoke\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sos i had to use\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoke\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-e839f8b54fd3>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, tokenizer, seed_text, maxlen)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#y_class = y_prob.argmax(axis=-1)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#print(y_prob)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(y_class)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: sum(pvals[:-1]) > 1.0"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos i had to use\", maxlen=40)\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sos hello', 'hello ,', \", i'm\", \"i'm a\", 'a dinosaur', 'dinosaur .', '. eos']\n"
     ]
    }
   ],
   "source": [
    "def bigrams_list(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    bigrams = []\n",
    "    for i in range(0, len(words)-1):\n",
    "        bigrams.append(words[i]+' '+words[i+1])\n",
    "    return bigrams\n",
    "\n",
    "print(bigrams_list(\"sos hello , i'm a dinosaur . eos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sos how', 'how does', 'does an', 'an alabama', 'alabama girl', 'girl know', \"know she's\", \"she's in\", 'in for', 'for a', 'a crazy', 'crazy night', 'night ?', '? her', 'her daddy', 'daddy says', 'says he', 'he wants', 'wants her', 'her in', 'in bed', 'bed by', 'by ten', 'ten .', '. eos'], ['sos my', 'my friend', 'friend works', 'works at', 'at a', 'a circumcision', 'circumcision clinic', 'clinic i', 'i asked', 'asked him', 'him if', 'if he', 'he charges', 'charges alot', 'alot for', 'for his', 'his circumcisions', 'circumcisions he', 'he said', 'said \"', '\" no', 'no ,', ', i', 'i just', 'just keep', 'keep the', 'the tips', 'tips .', '. \"', '\" eos']]\n"
     ]
    }
   ],
   "source": [
    "sentence_bigrams = [bigrams_list(s) for s in sentences]\n",
    "print(sentence_bigrams[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    temp = set(lst2)\n",
    "    lst3 = [value for value in lst1 if value in temp]\n",
    "    return lst3\n",
    "\n",
    "def similarity_score(lst1, lst2):\n",
    "    intersection_len = len(intersection(lst1, lst2))\n",
    "    return (1.0*intersection_len)/len(lst1)#+len(lst2)-intersection_len)\n",
    " \n",
    "def print_closest_sentences(sentence, sentence_bigrams, top_k=3):\n",
    "    bigrams = bigrams_list(sentence)\n",
    "    scores = np.array([similarity_score(bigrams, sbigrams)\n",
    "                       for sbigrams in sentence_bigrams])\n",
    "    top_k_indices = scores.argsort()[-1*top_k:][::-1]\n",
    "    top_k_scores = scores[top_k_indices]\n",
    "    for k in range(top_k):\n",
    "        print(top_k_scores[k], \" -> \", sentences[top_k_indices[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666  ->  sos i bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6666666666666666  ->  sos i bought a pair of shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6666666666666666  ->  sos i recently bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6666666666666666  ->  sos i bought some shoes from my dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos my drug dealer gave me new shoes today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i recently bought some shoes from a drug dealer threedots i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i bought shoes from my drug dealer today i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i recently bought shoes from a drug dealer i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos so i bought some shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i bought a pair of shoes from a drug dealer threedots i don't know what he laced them with , but i've been tripping all day . eos\n"
     ]
    }
   ],
   "source": [
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 17, 20, 9, 66]\n",
      "sos what do you call a mexican midget ? a paragraph , because he's not a full essay . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph , because he's not a full essay eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph , because he's not a full ese . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph because he's not a full essay . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph , because he is not a full essay . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican with no legs ? a paragraph , because he's not a full essay . eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph , because he's too short to be an essay . eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph , because he's too short to be an essay . eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph threedots because he's not a full essay eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph , because he is not a full essay ! eos\n",
      "0.7894736842105263  ->  sos what do you call a spanish midget ? a paragraph . because he's not a full essay . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos what do you call\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"sos a woman was arrested when her boyfriend's body was found in a freezer in their living room . who the hell puts a freezer in the living room ? eos\", 'sos what does heroin make you feel like ? more heroin . eos', \"sos why couldn't the physicist understand how boats work ? he thought nothing could possibly travel faster than sea . eos\", \"sos at what age do you tell a highway it's adopted ? eos\", \"sos russians dolls . they're so full of themselves eos\", \"sos how many chocolate bunnies can you put into an empty easter basket ? one . after that the basket won't be empty . eos\", \"sos nurse pops her head into the doctor's office threedots nurse : ' doctor , there's an invisible man in the waiting room . ' doctor : ' tell him i can't see him . ' eos\", 'sos what kind of file makes a hole bigger ? a pedophile eos', \"sos what is a paranoid man's favorite food ? who wants to know ? eos\", \"sos friends invited me to a meteor shower party , but i couldn't make it . they were crushed . eos\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 5, 76, 74, 167]\n",
      "sos what's the difference between being a pedophile and a rapist ? a pedophile comes up to a bar . eos\n",
      "0.4  ->  sos a rapist , a pedophile and a priest walks into a bar . eos\n",
      "0.4  ->  sos a priest , a rapist , and a pedophile walk in to a bar . and he orders a drink . eos\n",
      "0.4  ->  sos what's the difference between a peeping tom and a rapist ? a rapist doesn't waste time beating around the bush eos\n",
      "0.4  ->  sos a priest , a pedophile and a rapist walk into a bar . he orders a drink . eos\n",
      "0.4  ->  sos a rapist , a pedophile and a priest walk into a bar . he orders a drink . eos\n",
      "0.35  ->  sos what's the difference between a slut and a bitch ? a slut fucks everyone threedots a bitch fucks everyone but you . eos\n",
      "0.35  ->  sos what's the difference between a bmw driver and a porcupine ? a porcupine has the pricks on the outside . eos\n",
      "0.35  ->  sos what's the difference between a blonde and a mosquito ? a mosquito will stop sucking after you slap it . eos\n",
      "0.35  ->  sos a priest , a rapist and a pedophile walks into a bar . he sits down and has a drink . eos\n",
      "0.35  ->  sos what's the difference between a vegetarian and a virgin ? a vegetarian doesn't like meat and a virgin doesn't know yet if she likes meat or not . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos what's the difference between being\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 =  load_model('models/checkpoints/jokes_bilstm_gen2.08-4.39.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 107, 1832]\n",
      "sos a guy finds a rectal thermometer in his pocket and says \" i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i recently bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i recently bought shoes from a drug dealer i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos my drug dealer gave me new shoes today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought shoes from my drug dealer today i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos shoes from a drug dealer i bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i just bought shoes from a drug dealer threedots threedots i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought some shoes from my dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos so i bought some shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought a pair of shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos a guy finds\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
