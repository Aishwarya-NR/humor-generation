{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import importlib\n",
    "from library import data_preprocess as dp\n",
    "importlib.reload(dp)\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Input\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Embedding, TimeDistributed, Flatten, Merge, Concatenate\n",
    "from keras import regularizers\n",
    "from keras.metrics import sparse_categorical_accuracy, sparse_categorical_crossentropy\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './datasets/jokes.pickle'\n",
    "VOCAB_PATH = './datasets/jokes_vocabulary.pickle'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 13\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "MODEL_PREFIX = 'jokes_stacked_lstm'\n",
    "\n",
    "EMBEDDING_DIM = 512\n",
    "HIDDEN_DIM1 = 1024\n",
    "HIDDEN_DIM2 = 512\n",
    "DEEPER_DIM = 512\n",
    "DROPOUT_FACTOR = 0.2\n",
    "REGULARIZATION = 0.00001\n",
    "LEARNING_RATE = 0.003\n",
    "\n",
    "DATA_PERCENT = 0.1\n",
    "\n",
    "RUN_INDEX = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences =  96910\n",
      "[\"sos my credit card was stolen today i don't think i'm gonna do anything about it . so far he's spending way less than my wife does . eos\", \"sos if i ever start with ' this one time i went jogging threedots ' i am not telling the truth . eos\"]\n",
      "Vocab size =  8922\n",
      "['sos', 'did', 'you', 'hear', 'about', 'the', 'new', 'corduroy', 'pillows', '?']\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as pickleFile:\n",
    "    sentences = pickle.load(pickleFile)\n",
    "\n",
    "with open(VOCAB_PATH, 'rb') as pickleFile:\n",
    "    vocab = pickle.load(pickleFile)\n",
    "    \n",
    "random.shuffle(sentences)\n",
    "\n",
    "print(\"Number of sentences = \", len(sentences))\n",
    "print(sentences[:2])\n",
    "print(\"Vocab size = \", len(vocab))\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 13, 1173, 928, 24, 1245, 136, 7, 47, 106, 45, 348, 20, 298, 51, 16, 3, 43, 699, 179, 2804, 150, 780, 151, 13, 108, 64, 3, 2], [1, 38, 7, 143, 314, 29, 81, 69, 46, 90, 7, 190, 4552, 15, 81, 7, 237, 58, 508, 5, 1880, 3, 2], [1, 17, 36, 46, 664, 73, 60, 8, 5, 91, 6, 802, 33, 1984, 2], [1, 217, 23, 56, 174, 4222, 279, 787, 6, 42, 4, 23, 253, 299, 2467, 21, 2], [1, 17, 36, 5, 2935, 2059, 1012, 60, 6, 114, 9, 1356, 22, 6, 2]]\n",
      "8923\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "num_words = len(vocab)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, filters='', lower=True, split=' ', \n",
    "                      char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "assert num_words == len(tokenizer.word_index)\n",
    "\n",
    "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded_sentences[:5])\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open(MODELS_PATH + MODEL_PREFIX + '_tokenizer_' + str(RUN_INDEX) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training data size =  833131\n",
      "Max seq len =  13\n",
      "(833131, 13)\n",
      "[[   1   13 1173  928   24 1245  136    7   47  106   45  348   20]\n",
      " [  13 1173  928   24 1245  136    7   47  106   45  348   20  298]]\n",
      "(833131, 13, 1)\n",
      "[[[  13]\n",
      "  [1173]\n",
      "  [ 928]\n",
      "  [  24]\n",
      "  [1245]\n",
      "  [ 136]\n",
      "  [   7]\n",
      "  [  47]\n",
      "  [ 106]\n",
      "  [  45]\n",
      "  [ 348]\n",
      "  [  20]\n",
      "  [ 298]]\n",
      "\n",
      " [[1173]\n",
      "  [ 928]\n",
      "  [  24]\n",
      "  [1245]\n",
      "  [ 136]\n",
      "  [   7]\n",
      "  [  47]\n",
      "  [ 106]\n",
      "  [  45]\n",
      "  [ 348]\n",
      "  [  20]\n",
      "  [ 298]\n",
      "  [  51]]]\n"
     ]
    }
   ],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "for sentence in encoded_sentences:\n",
    "    l = len(sentence)\n",
    "    sliding_window_length = min(l-3, MAX_SEQUENCE_LENGTH)\n",
    "    step_size = 1\n",
    "    for i in range(0, l - sliding_window_length, step_size):\n",
    "        X_data.append(sentence[i:i+sliding_window_length])\n",
    "        y_data.append(sentence[i+1:i+sliding_window_length+1])\n",
    "        \n",
    "print(\"Total training data size = \", len(X_data))\n",
    "MAX_SEQ_LEN = max([len(seq) for seq in X_data])\n",
    "print(\"Max seq len = \", MAX_SEQ_LEN)\n",
    "X_data = pad_sequences(X_data, maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "y_data = pad_sequences(y_data, maxlen=MAX_SEQ_LEN, padding='pre').reshape(-1, MAX_SEQ_LEN, 1)\n",
    "#y_data = np.array(y_data).reshape(-1,1)\n",
    "print(X_data.shape)\n",
    "print(X_data[:2])\n",
    "print(y_data.shape)\n",
    "print(y_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def StackedLSTM(vocab_size, embedding_dim, hidden_dim1, hidden_dim2, deeper_dim, max_seq_len, \n",
    "           dropout_factor=0.5, regularization=0.00001, learning_rate=0.001):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, #input_length=max_seq_len, \n",
    "                        mask_zero=True, embeddings_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(LSTM(hidden_dim1, activation='tanh', \n",
    "                   kernel_regularizer=regularizers.l2(regularization), \n",
    "                   recurrent_regularizer=regularizers.l2(regularization), #unroll=True, \n",
    "                   return_sequences = True, dropout=dropout_factor, recurrent_dropout=dropout_factor))\n",
    "    model.add(LSTM(hidden_dim2, activation='tanh', \n",
    "                   kernel_regularizer=regularizers.l2(regularization), \n",
    "                   recurrent_regularizer=regularizers.l2(regularization), #unroll=True, \n",
    "                   return_sequences = True, dropout=dropout_factor, recurrent_dropout=dropout_factor))\n",
    "    model.add(TimeDistributed(Dropout(dropout_factor)))\n",
    "    model.add(Dense(units=deeper_dim, activation='tanh', kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dropout(dropout_factor))\n",
    "    model.add(Dense(units=vocab_size, activation='softmax', \n",
    "              kernel_regularizer=regularizers.l2(regularization)))\n",
    "    #model = multi_gpu_model(model)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=learning_rate),\n",
    "                  metrics=[sparse_categorical_crossentropy, sparse_categorical_accuracy], sample_weight_mode='temporal')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 512)         4568576   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 1024)        6295552   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 512)         3147776   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 512)         262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 8923)        4577499   \n",
      "=================================================================\n",
      "Total params: 18,852,059\n",
      "Trainable params: 18,852,059\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "model = StackedLSTM(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, hidden_dim1=HIDDEN_DIM1, hidden_dim2=HIDDEN_DIM2, \n",
    "                    deeper_dim=DEEPER_DIM, max_seq_len=MAX_SEQ_LEN, dropout_factor=DROPOUT_FACTOR, \n",
    "                    regularization=REGULARIZATION, learning_rate=LEARNING_RATE)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TB(TensorBoard):\n",
    "    def __init__(self, log_every=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.log_every = log_every\n",
    "        self.counter = 0\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.counter+=1\n",
    "        if self.counter%self.log_every==0:\n",
    "            for name, value in logs.items():\n",
    "                if name in ['batch', 'size']:\n",
    "                    continue\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.writer.add_summary(summary, self.counter)\n",
    "            self.writer.flush()\n",
    "        \n",
    "        super().on_batch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666504 samples, validate on 166627 samples\n",
      "Epoch 1/25\n",
      "666504/666504 [==============================] - 853s 1ms/step - loss: 5.7040 - sparse_categorical_crossentropy: 5.5896 - sparse_categorical_accuracy: 0.1210 - val_loss: 4.7178 - val_sparse_categorical_crossentropy: 4.4849 - val_sparse_categorical_accuracy: 0.2357\n",
      "\n",
      "Epoch 00001: saving model to ./models/checkpoints/jokes_stacked_lstm_gen6.01-4.72.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.71779, saving model to ./models/jokes_stacked_lstm_gen6.hdf5\n",
      "Epoch 2/25\n",
      "666504/666504 [==============================] - 857s 1ms/step - loss: 4.4894 - sparse_categorical_crossentropy: 4.1798 - sparse_categorical_accuracy: 0.2551 - val_loss: 4.5102 - val_sparse_categorical_crossentropy: 4.1451 - val_sparse_categorical_accuracy: 0.2765\n",
      "\n",
      "Epoch 00002: saving model to ./models/checkpoints/jokes_stacked_lstm_gen6.02-4.51.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.71779 to 4.51020, saving model to ./models/jokes_stacked_lstm_gen6.hdf5\n",
      "Epoch 3/25\n",
      "666504/666504 [==============================] - 860s 1ms/step - loss: 4.2997 - sparse_categorical_crossentropy: 3.8992 - sparse_categorical_accuracy: 0.2841 - val_loss: 4.4628 - val_sparse_categorical_crossentropy: 4.0370 - val_sparse_categorical_accuracy: 0.2916\n",
      "\n",
      "Epoch 00003: saving model to ./models/checkpoints/jokes_stacked_lstm_gen6.03-4.46.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.51020 to 4.46278, saving model to ./models/jokes_stacked_lstm_gen6.hdf5\n",
      "Epoch 4/25\n",
      "666504/666504 [==============================] - 858s 1ms/step - loss: 4.2164 - sparse_categorical_crossentropy: 3.7684 - sparse_categorical_accuracy: 0.2981 - val_loss: 4.4436 - val_sparse_categorical_crossentropy: 3.9822 - val_sparse_categorical_accuracy: 0.2998\n",
      "\n",
      "Epoch 00004: saving model to ./models/checkpoints/jokes_stacked_lstm_gen6.04-4.44.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.46278 to 4.44359, saving model to ./models/jokes_stacked_lstm_gen6.hdf5\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tensorboard = TB(log_dir=\"./logs/\" + MODEL_PREFIX + \"/{}\".format(time()), \n",
    "                          histogram_freq=0, write_graph=True, write_images=False, log_every=10)\n",
    "\n",
    "callbacks=[tensorboard, \n",
    "           EarlyStopping(patience=5, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=MODELS_PATH + 'checkpoints/'+ MODEL_PREFIX + '_gen'+str(RUN_INDEX)+'.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=1), \n",
    "           ModelCheckpoint(filepath=MODELS_PATH + MODEL_PREFIX + '_gen'+str(RUN_INDEX)+'.hdf5', \n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=1, save_best_only=True)]\n",
    "\n",
    "model.fit(X_data, y_data, epochs=25, batch_size=1024, shuffle=True, verbose=1, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "print(\"Total elapsed time: \", time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate(model, tokenizer, seed_text, maxlen, probabilistic=False, exploration_factor=1.0):\n",
    "    \n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    print(seq)\n",
    "    \n",
    "    while True:\n",
    "        encoded_seq = seq\n",
    "        if len(seq) > MAX_SEQ_LEN:\n",
    "            encoded_seq = encoded_seq[-1*MAX_SEQ_LEN:]\n",
    "            \n",
    "        #padded_seq = pad_sequences([encoded_seq], maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "        padded_seq = np.array([seq])\n",
    "        y_prob = model.predict(padded_seq)[0][-1].reshape(1,-1)#[3:].reshape(-1,1)\n",
    "        \n",
    "        if random.random() <= exploration_factor:\n",
    "            probabilistic = True\n",
    "        else:\n",
    "            probabilistic = False\n",
    "            \n",
    "        if probabilistic:\n",
    "            y_class = np.argmax(np.random.multinomial(1,y_prob[0]/(np.sum(y_prob[0])+1e-5),1))\n",
    "        else:\n",
    "            y_class = y_prob.argmax(axis=-1)[0]\n",
    "        \n",
    "        if y_class == 0:\n",
    "            break\n",
    "        out_word = reverse_word_map[y_class]\n",
    "        seq.append(y_class)\n",
    "        if out_word == 'eos' or len(seq) > maxlen or out_word == 'sos':\n",
    "            break\n",
    "    \n",
    "    words = [reverse_word_map[idx] for idx in seq]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/jokes_stacked_lstm_gen5.hdf5')\n",
    "with open('models/jokes_tokenizer_5.pickle', 'rb') as pickleFile:\n",
    "    tokenizer = pickle.load(pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 7, 84, 8, 258]\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "sum(pvals[:-1]) > 1.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6df94c8249b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoke\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sos i had to use\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoke\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-e839f8b54fd3>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, tokenizer, seed_text, maxlen)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#y_class = y_prob.argmax(axis=-1)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#print(y_prob)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(y_class)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: sum(pvals[:-1]) > 1.0"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos i had to use\", maxlen=40)\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sos hello', 'hello ,', \", i'm\", \"i'm a\", 'a dinosaur', 'dinosaur .', '. eos']\n"
     ]
    }
   ],
   "source": [
    "def bigrams_list(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    bigrams = []\n",
    "    for i in range(0, len(words)-1):\n",
    "        bigrams.append(words[i]+' '+words[i+1])\n",
    "    return bigrams\n",
    "\n",
    "print(bigrams_list(\"sos hello , i'm a dinosaur . eos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sos how', 'how does', 'does an', 'an alabama', 'alabama girl', 'girl know', \"know she's\", \"she's in\", 'in for', 'for a', 'a crazy', 'crazy night', 'night ?', '? her', 'her daddy', 'daddy says', 'says he', 'he wants', 'wants her', 'her in', 'in bed', 'bed by', 'by ten', 'ten .', '. eos'], ['sos my', 'my friend', 'friend works', 'works at', 'at a', 'a circumcision', 'circumcision clinic', 'clinic i', 'i asked', 'asked him', 'him if', 'if he', 'he charges', 'charges alot', 'alot for', 'for his', 'his circumcisions', 'circumcisions he', 'he said', 'said \"', '\" no', 'no ,', ', i', 'i just', 'just keep', 'keep the', 'the tips', 'tips .', '. \"', '\" eos']]\n"
     ]
    }
   ],
   "source": [
    "sentence_bigrams = [bigrams_list(s) for s in sentences]\n",
    "print(sentence_bigrams[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    temp = set(lst2)\n",
    "    lst3 = [value for value in lst1 if value in temp]\n",
    "    return lst3\n",
    "\n",
    "def similarity_score(lst1, lst2):\n",
    "    intersection_len = len(intersection(lst1, lst2))\n",
    "    return (1.0*intersection_len)/len(lst1)#+len(lst2)-intersection_len)\n",
    " \n",
    "def print_closest_sentences(sentence, sentence_bigrams, top_k=3):\n",
    "    bigrams = bigrams_list(sentence)\n",
    "    scores = np.array([similarity_score(bigrams, sbigrams)\n",
    "                       for sbigrams in sentence_bigrams])\n",
    "    top_k_indices = scores.argsort()[-1*top_k:][::-1]\n",
    "    top_k_scores = scores[top_k_indices]\n",
    "    for k in range(top_k):\n",
    "        print(top_k_scores[k], \" -> \", sentences[top_k_indices[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666  ->  sos i bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6666666666666666  ->  sos i bought a pair of shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6666666666666666  ->  sos i recently bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6666666666666666  ->  sos i bought some shoes from my dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos my drug dealer gave me new shoes today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i recently bought some shoes from a drug dealer threedots i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i bought shoes from my drug dealer today i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i recently bought shoes from a drug dealer i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos so i bought some shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i bought a pair of shoes from a drug dealer threedots i don't know what he laced them with , but i've been tripping all day . eos\n"
     ]
    }
   ],
   "source": [
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 17, 20, 9, 66]\n",
      "sos what do you call a mexican midget ? a paragraph , because he's not a full essay . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph , because he's not a full essay eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph , because he's not a full ese . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph because he's not a full essay . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph , because he is not a full essay . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican with no legs ? a paragraph , because he's not a full essay . eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph , because he's too short to be an essay . eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph , because he's too short to be an essay . eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph threedots because he's not a full essay eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph , because he is not a full essay ! eos\n",
      "0.7894736842105263  ->  sos what do you call a spanish midget ? a paragraph . because he's not a full essay . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos what do you call\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"sos a woman was arrested when her boyfriend's body was found in a freezer in their living room . who the hell puts a freezer in the living room ? eos\", 'sos what does heroin make you feel like ? more heroin . eos', \"sos why couldn't the physicist understand how boats work ? he thought nothing could possibly travel faster than sea . eos\", \"sos at what age do you tell a highway it's adopted ? eos\", \"sos russians dolls . they're so full of themselves eos\", \"sos how many chocolate bunnies can you put into an empty easter basket ? one . after that the basket won't be empty . eos\", \"sos nurse pops her head into the doctor's office threedots nurse : ' doctor , there's an invisible man in the waiting room . ' doctor : ' tell him i can't see him . ' eos\", 'sos what kind of file makes a hole bigger ? a pedophile eos', \"sos what is a paranoid man's favorite food ? who wants to know ? eos\", \"sos friends invited me to a meteor shower party , but i couldn't make it . they were crushed . eos\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 5, 76, 74, 167]\n",
      "sos what's the difference between being a pedophile and a rapist ? a pedophile comes up to a bar . eos\n",
      "0.4  ->  sos a rapist , a pedophile and a priest walks into a bar . eos\n",
      "0.4  ->  sos a priest , a rapist , and a pedophile walk in to a bar . and he orders a drink . eos\n",
      "0.4  ->  sos what's the difference between a peeping tom and a rapist ? a rapist doesn't waste time beating around the bush eos\n",
      "0.4  ->  sos a priest , a pedophile and a rapist walk into a bar . he orders a drink . eos\n",
      "0.4  ->  sos a rapist , a pedophile and a priest walk into a bar . he orders a drink . eos\n",
      "0.35  ->  sos what's the difference between a slut and a bitch ? a slut fucks everyone threedots a bitch fucks everyone but you . eos\n",
      "0.35  ->  sos what's the difference between a bmw driver and a porcupine ? a porcupine has the pricks on the outside . eos\n",
      "0.35  ->  sos what's the difference between a blonde and a mosquito ? a mosquito will stop sucking after you slap it . eos\n",
      "0.35  ->  sos a priest , a rapist and a pedophile walks into a bar . he sits down and has a drink . eos\n",
      "0.35  ->  sos what's the difference between a vegetarian and a virgin ? a vegetarian doesn't like meat and a virgin doesn't know yet if she likes meat or not . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos what's the difference between being\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 =  load_model('models/checkpoints/jokes_bilstm_gen2.08-4.39.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 107, 1832]\n",
      "sos a guy finds a rectal thermometer in his pocket and says \" i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i recently bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i recently bought shoes from a drug dealer i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos my drug dealer gave me new shoes today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought shoes from my drug dealer today i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos shoes from a drug dealer i bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i just bought shoes from a drug dealer threedots threedots i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought some shoes from my dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos so i bought some shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought a pair of shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos a guy finds\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
