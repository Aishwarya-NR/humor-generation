{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import importlib\n",
    "from library import data_preprocess as dp\n",
    "importlib.reload(dp)\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Embedding, TimeDistributed, Flatten\n",
    "from keras import regularizers\n",
    "from keras.metrics import sparse_categorical_accuracy, sparse_categorical_crossentropy\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './datasets/jokes.pickle'\n",
    "VOCAB_PATH = './datasets/jokes_vocabulary.pickle'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 13\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM1 = 512\n",
    "HIDDEN_DIM2 = 256\n",
    "DEEPER_DIM = 256\n",
    "DROPOUT_FACTOR = 0.25\n",
    "REGULARIZATION = 0.00001\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "DATA_PERCENT = 0.1\n",
    "\n",
    "RUN_INDEX = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences =  96910\n",
      "[\"sos how does an alabama girl know she's in for a crazy night ? her daddy says he wants her in bed by ten . eos\", 'sos my friend works at a circumcision clinic i asked him if he charges alot for his circumcisions he said \" no , i just keep the tips . \" eos']\n",
      "Vocab size =  8922\n",
      "['sos', 'did', 'you', 'hear', 'about', 'the', 'new', 'corduroy', 'pillows', '?']\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as pickleFile:\n",
    "    sentences = pickle.load(pickleFile)\n",
    "\n",
    "with open(VOCAB_PATH, 'rb') as pickleFile:\n",
    "    vocab = pickle.load(pickleFile)\n",
    "    \n",
    "random.shuffle(sentences)\n",
    "\n",
    "print(\"Number of sentences = \", len(sentences))\n",
    "print(sentences[:2])\n",
    "print(\"Vocab size = \", len(vocab))\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 32, 64, 50, 3028, 124, 67, 294, 14, 30, 4, 712, 168, 6, 61, 1187, 105, 31, 446, 61, 14, 323, 122, 609, 3, 2], [1, 13, 157, 840, 49, 4, 2059, 2342, 7, 149, 103, 38, 31, 4676, 3927, 30, 53, 5734, 31, 79, 11, 62, 10, 7, 44, 218, 5, 2314, 3, 11, 2], [1, 144, 144, 268, 75, 21, 6156, 21, 6156, 71, 6, 6156, 689, 2366, 5, 7215, 21, 2], [1, 7, 190, 8, 5, 201, 29, 7515, 1604, 10, 31, 1424, 93, 31, 408, 13, 2261, 4019, 3, 7, 106, 179, 527, 5, 1311, 41, 7, 180, 240, 181, 4, 164, 3, 2], [1, 13, 141, 12, 7, 39, 192, 720, 41, 45, 230, 666, 45, 4, 107, 2]]\n",
      "8923\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "num_words = len(vocab)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, filters='', lower=True, split=' ', \n",
    "                      char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "assert num_words == len(tokenizer.word_index)\n",
    "\n",
    "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded_sentences[:5])\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open(MODELS_PATH + 'jokes_tokenizer_' + str(RUN_INDEX) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training data size =  833131\n",
      "Max seq len =  13\n",
      "(833131, 13)\n",
      "[[   1   32   64   50 3028  124   67  294   14   30    4  712  168]\n",
      " [  32   64   50 3028  124   67  294   14   30    4  712  168    6]]\n",
      "(833131, 13, 1)\n",
      "[[[  32]\n",
      "  [  64]\n",
      "  [  50]\n",
      "  [3028]\n",
      "  [ 124]\n",
      "  [  67]\n",
      "  [ 294]\n",
      "  [  14]\n",
      "  [  30]\n",
      "  [   4]\n",
      "  [ 712]\n",
      "  [ 168]\n",
      "  [   6]]\n",
      "\n",
      " [[  64]\n",
      "  [  50]\n",
      "  [3028]\n",
      "  [ 124]\n",
      "  [  67]\n",
      "  [ 294]\n",
      "  [  14]\n",
      "  [  30]\n",
      "  [   4]\n",
      "  [ 712]\n",
      "  [ 168]\n",
      "  [   6]\n",
      "  [  61]]]\n"
     ]
    }
   ],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "for sentence in encoded_sentences:\n",
    "    l = len(sentence)\n",
    "    sliding_window_length = min(l-3, MAX_SEQUENCE_LENGTH)\n",
    "    step_size = 1\n",
    "    for i in range(0, l - sliding_window_length, step_size):\n",
    "        X_data.append(sentence[i:i+sliding_window_length])\n",
    "        y_data.append(sentence[i+1:i+sliding_window_length+1])\n",
    "        \n",
    "print(\"Total training data size = \", len(X_data))\n",
    "MAX_SEQ_LEN = max([len(seq) for seq in X_data])\n",
    "print(\"Max seq len = \", MAX_SEQ_LEN)\n",
    "X_data = pad_sequences(X_data, maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "y_data = pad_sequences(y_data, maxlen=MAX_SEQ_LEN, padding='pre').reshape(-1, MAX_SEQ_LEN, 1)\n",
    "#y_data = np.array(y_data).reshape(-1,1)\n",
    "print(X_data.shape)\n",
    "print(X_data[:2])\n",
    "print(y_data.shape)\n",
    "print(y_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def BiLSTM(vocab_size, embedding_dim, hidden_dim1, hidden_dim2, deeper_dim, max_seq_len, \n",
    "           dropout_factor=0.5, regularization=0.00001, learning_rate=0.001):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, #input_length=max_seq_len, \n",
    "                        mask_zero=True, embeddings_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(LSTM(hidden_dim1, activation='tanh', \n",
    "                   kernel_regularizer=regularizers.l2(regularization), \n",
    "                   recurrent_regularizer=regularizers.l2(regularization), #unroll=True, \n",
    "                   return_sequences = True, dropout=dropout_factor, recurrent_dropout=dropout_factor))\n",
    "    model.add(LSTM(hidden_dim2, activation='tanh', \n",
    "                   kernel_regularizer=regularizers.l2(regularization), \n",
    "                   recurrent_regularizer=regularizers.l2(regularization), #unroll=True, \n",
    "                   return_sequences = True, dropout=dropout_factor, recurrent_dropout=dropout_factor))\n",
    "    model.add(TimeDistributed(Dropout(dropout_factor)))\n",
    "    model.add(Dense(units=deeper_dim, activation='tanh', kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Dense(units=vocab_size, activation='softmax', \n",
    "              kernel_regularizer=regularizers.l2(regularization)))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=learning_rate),\n",
    "                  metrics=[sparse_categorical_crossentropy, sparse_categorical_accuracy], sample_weight_mode='temporal')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 256)         2284288   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 512)         1574912   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 256)         787456    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 256)         65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 8923)        2293211   \n",
      "=================================================================\n",
      "Total params: 7,005,659\n",
      "Trainable params: 7,005,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, hidden_dim1=HIDDEN_DIM1, hidden_dim2=HIDDEN_DIM2,\n",
    "               deeper_dim=DEEPER_DIM, max_seq_len=MAX_SEQ_LEN, dropout_factor=DROPOUT_FACTOR, \n",
    "               regularization=REGULARIZATION, learning_rate=LEARNING_RATE)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TB(TensorBoard):\n",
    "    def __init__(self, log_every=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.log_every = log_every\n",
    "        self.counter = 0\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.counter+=1\n",
    "        if self.counter%self.log_every==0:\n",
    "            for name, value in logs.items():\n",
    "                if name in ['batch', 'size']:\n",
    "                    continue\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.writer.add_summary(summary, self.counter)\n",
    "            self.writer.flush()\n",
    "        \n",
    "        super().on_batch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666504 samples, validate on 166627 samples\n",
      "Epoch 1/20\n",
      "666504/666504 [==============================] - 508s 762us/step - loss: 6.0128 - sparse_categorical_crossentropy: 5.9758 - sparse_categorical_accuracy: 0.0863 - val_loss: 5.1412 - val_sparse_categorical_crossentropy: 5.0665 - val_sparse_categorical_accuracy: 0.1770\n",
      "\n",
      "Epoch 00001: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.01-5.14.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.14123, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 2/20\n",
      "666504/666504 [==============================] - 514s 772us/step - loss: 4.7715 - sparse_categorical_crossentropy: 4.6489 - sparse_categorical_accuracy: 0.2114 - val_loss: 4.6142 - val_sparse_categorical_crossentropy: 4.4527 - val_sparse_categorical_accuracy: 0.2395\n",
      "\n",
      "Epoch 00002: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.02-4.61.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.14123 to 4.61422, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 3/20\n",
      "666504/666504 [==============================] - 515s 772us/step - loss: 4.4611 - sparse_categorical_crossentropy: 4.2719 - sparse_categorical_accuracy: 0.2472 - val_loss: 4.4901 - val_sparse_categorical_crossentropy: 4.2786 - val_sparse_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00003: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.03-4.49.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.61422 to 4.49015, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 4/20\n",
      "666504/666504 [==============================] - 513s 770us/step - loss: 4.3405 - sparse_categorical_crossentropy: 4.1102 - sparse_categorical_accuracy: 0.2638 - val_loss: 4.4337 - val_sparse_categorical_crossentropy: 4.1886 - val_sparse_categorical_accuracy: 0.2719\n",
      "\n",
      "Epoch 00004: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.04-4.43.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.49015 to 4.43371, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 5/20\n",
      "666504/666504 [==============================] - 514s 771us/step - loss: 4.2719 - sparse_categorical_crossentropy: 4.0133 - sparse_categorical_accuracy: 0.2742 - val_loss: 4.4045 - val_sparse_categorical_crossentropy: 4.1354 - val_sparse_categorical_accuracy: 0.2791\n",
      "\n",
      "Epoch 00005: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.05-4.40.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.43371 to 4.40449, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 6/20\n",
      "666504/666504 [==============================] - 514s 771us/step - loss: 4.2252 - sparse_categorical_crossentropy: 3.9457 - sparse_categorical_accuracy: 0.2814 - val_loss: 4.3822 - val_sparse_categorical_crossentropy: 4.0952 - val_sparse_categorical_accuracy: 0.2848\n",
      "\n",
      "Epoch 00006: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.06-4.38.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.40449 to 4.38225, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 7/20\n",
      "666504/666504 [==============================] - 514s 770us/step - loss: 4.1901 - sparse_categorical_crossentropy: 3.8947 - sparse_categorical_accuracy: 0.2870 - val_loss: 4.3680 - val_sparse_categorical_crossentropy: 4.0665 - val_sparse_categorical_accuracy: 0.2885\n",
      "\n",
      "Epoch 00007: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.07-4.37.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.38225 to 4.36800, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 8/20\n",
      "666504/666504 [==============================] - 514s 771us/step - loss: 4.1628 - sparse_categorical_crossentropy: 3.8546 - sparse_categorical_accuracy: 0.2913 - val_loss: 4.3567 - val_sparse_categorical_crossentropy: 4.0439 - val_sparse_categorical_accuracy: 0.2913\n",
      "\n",
      "Epoch 00008: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.08-4.36.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.36800 to 4.35673, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 9/20\n",
      "666504/666504 [==============================] - 514s 771us/step - loss: 4.1403 - sparse_categorical_crossentropy: 3.8217 - sparse_categorical_accuracy: 0.2949 - val_loss: 4.3497 - val_sparse_categorical_crossentropy: 4.0275 - val_sparse_categorical_accuracy: 0.2939\n",
      "\n",
      "Epoch 00009: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.09-4.35.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.35673 to 4.34975, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 10/20\n",
      "666504/666504 [==============================] - 514s 772us/step - loss: 4.1218 - sparse_categorical_crossentropy: 3.7946 - sparse_categorical_accuracy: 0.2981 - val_loss: 4.3412 - val_sparse_categorical_crossentropy: 4.0110 - val_sparse_categorical_accuracy: 0.2962\n",
      "\n",
      "Epoch 00010: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.10-4.34.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 4.34975 to 4.34116, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 11/20\n",
      "666504/666504 [==============================] - 513s 770us/step - loss: 4.1054 - sparse_categorical_crossentropy: 3.7707 - sparse_categorical_accuracy: 0.3007 - val_loss: 4.3386 - val_sparse_categorical_crossentropy: 4.0017 - val_sparse_categorical_accuracy: 0.2978\n",
      "\n",
      "Epoch 00011: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.11-4.34.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 4.34116 to 4.33860, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 12/20\n",
      "666504/666504 [==============================] - 514s 771us/step - loss: 4.0916 - sparse_categorical_crossentropy: 3.7506 - sparse_categorical_accuracy: 0.3028 - val_loss: 4.3339 - val_sparse_categorical_crossentropy: 3.9910 - val_sparse_categorical_accuracy: 0.2992\n",
      "\n",
      "Epoch 00012: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.12-4.33.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 4.33860 to 4.33387, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 13/20\n",
      "666504/666504 [==============================] - 514s 772us/step - loss: 4.0797 - sparse_categorical_crossentropy: 3.7332 - sparse_categorical_accuracy: 0.3048 - val_loss: 4.3259 - val_sparse_categorical_crossentropy: 3.9778 - val_sparse_categorical_accuracy: 0.3006\n",
      "\n",
      "Epoch 00013: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.13-4.33.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 4.33387 to 4.32587, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 14/20\n",
      "666504/666504 [==============================] - 514s 771us/step - loss: 4.0681 - sparse_categorical_crossentropy: 3.7167 - sparse_categorical_accuracy: 0.3066 - val_loss: 4.3260 - val_sparse_categorical_crossentropy: 3.9732 - val_sparse_categorical_accuracy: 0.3021\n",
      "\n",
      "Epoch 00014: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.14-4.33.hdf5\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "666504/666504 [==============================] - 515s 773us/step - loss: 4.0584 - sparse_categorical_crossentropy: 3.7026 - sparse_categorical_accuracy: 0.3082 - val_loss: 4.3235 - val_sparse_categorical_crossentropy: 3.9668 - val_sparse_categorical_accuracy: 0.3026\n",
      "\n",
      "Epoch 00015: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.15-4.32.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 4.32587 to 4.32345, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 16/20\n",
      "666504/666504 [==============================] - 514s 772us/step - loss: 4.0494 - sparse_categorical_crossentropy: 3.6897 - sparse_categorical_accuracy: 0.3096 - val_loss: 4.3195 - val_sparse_categorical_crossentropy: 3.9590 - val_sparse_categorical_accuracy: 0.3046\n",
      "\n",
      "Epoch 00016: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.16-4.32.hdf5\n",
      "\n",
      "Epoch 00016: val_loss improved from 4.32345 to 4.31945, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 17/20\n",
      "666504/666504 [==============================] - 516s 775us/step - loss: 4.0410 - sparse_categorical_crossentropy: 3.6778 - sparse_categorical_accuracy: 0.3111 - val_loss: 4.3203 - val_sparse_categorical_crossentropy: 3.9563 - val_sparse_categorical_accuracy: 0.3047\n",
      "\n",
      "Epoch 00017: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.17-4.32.hdf5\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "666504/666504 [==============================] - 514s 771us/step - loss: 4.0335 - sparse_categorical_crossentropy: 3.6670 - sparse_categorical_accuracy: 0.3122 - val_loss: 4.3174 - val_sparse_categorical_crossentropy: 3.9503 - val_sparse_categorical_accuracy: 0.3060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.18-4.32.hdf5\n",
      "\n",
      "Epoch 00018: val_loss improved from 4.31945 to 4.31735, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 19/20\n",
      "666504/666504 [==============================] - 513s 770us/step - loss: 4.0272 - sparse_categorical_crossentropy: 3.6577 - sparse_categorical_accuracy: 0.3133 - val_loss: 4.3170 - val_sparse_categorical_crossentropy: 3.9471 - val_sparse_categorical_accuracy: 0.3064\n",
      "\n",
      "Epoch 00019: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.19-4.32.hdf5\n",
      "\n",
      "Epoch 00019: val_loss improved from 4.31735 to 4.31702, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Epoch 20/20\n",
      "666504/666504 [==============================] - 514s 771us/step - loss: 4.0205 - sparse_categorical_crossentropy: 3.6483 - sparse_categorical_accuracy: 0.3143 - val_loss: 4.3118 - val_sparse_categorical_crossentropy: 3.9392 - val_sparse_categorical_accuracy: 0.3075\n",
      "\n",
      "Epoch 00020: saving model to ./models/checkpoints/jokes_stacked_lstm_gen5.20-4.31.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 4.31702 to 4.31181, saving model to ./models/jokes_stacked_lstm_gen5.hdf5\n",
      "Total elapsed time:  10290.730658531189\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "tensorboard = TB(log_dir=\"./logs/jokes_stacked_lstm/{}\".format(time()), \n",
    "                          histogram_freq=0, write_graph=True, write_images=False, log_every=10)\n",
    "\n",
    "callbacks=[tensorboard, \n",
    "           EarlyStopping(patience=5, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=MODELS_PATH + 'checkpoints/jokes_stacked_lstm_gen'+str(RUN_INDEX)+'.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=1), \n",
    "           ModelCheckpoint(filepath=MODELS_PATH + 'jokes_stacked_lstm_gen'+str(RUN_INDEX)+'.hdf5', \n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=1, save_best_only=True)]\n",
    "\n",
    "model.fit(X_data, y_data, epochs=20, batch_size=512, shuffle=True, verbose=1, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "print(\"Total elapsed time: \", time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate(model, tokenizer, seed_text, maxlen):\n",
    "    \n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    print(seq)\n",
    "    \n",
    "    while True:\n",
    "        if len(seq) > MAX_SEQ_LEN:\n",
    "            encoded_seq = seq[-1*MAX_SEQ_LEN:]\n",
    "        else:\n",
    "            encoded_seq = seq\n",
    "        padded_seq = pad_sequences([encoded_seq], maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "        #padded_seq = np.array([seq])\n",
    "        y_prob = model.predict(padded_seq)[0][-1].reshape(1,-1)#[3:].reshape(-1,1)\n",
    "        #print(y_prob.shape)\n",
    "        #y_class = y_prob.argmax(axis=-1)[0]\n",
    "        y_class = np.argmax(np.random.multinomial(1,y_prob.squeeze(axis=0),1))\n",
    "        #print(y_prob)\n",
    "        #print(y_class)\n",
    "        if y_class == 0:\n",
    "            break\n",
    "        out_word = reverse_word_map[y_class]\n",
    "        seq.append(y_class)\n",
    "        if out_word == 'eos' or len(seq) > maxlen:\n",
    "            break\n",
    "    \n",
    "    words = [reverse_word_map[idx] for idx in seq]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 7, 84, 8, 258]\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n",
      "(1, 8923)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "sum(pvals[:-1]) > 1.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6df94c8249b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoke\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sos i had to use\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoke\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-e839f8b54fd3>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, tokenizer, seed_text, maxlen)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#y_class = y_prob.argmax(axis=-1)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#print(y_prob)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(y_class)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: sum(pvals[:-1]) > 1.0"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos i had to use\", maxlen=40)\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sos hello', 'hello ,', \", i'm\", \"i'm a\", 'a dinosaur', 'dinosaur .', '. eos']\n"
     ]
    }
   ],
   "source": [
    "def bigrams_list(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    bigrams = []\n",
    "    for i in range(0, len(words)-1):\n",
    "        bigrams.append(words[i]+' '+words[i+1])\n",
    "    return bigrams\n",
    "\n",
    "print(bigrams_list(\"sos hello , i'm a dinosaur . eos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sos how', 'how does', 'does an', 'an alabama', 'alabama girl', 'girl know', \"know she's\", \"she's in\", 'in for', 'for a', 'a crazy', 'crazy night', 'night ?', '? her', 'her daddy', 'daddy says', 'says he', 'he wants', 'wants her', 'her in', 'in bed', 'bed by', 'by ten', 'ten .', '. eos'], ['sos my', 'my friend', 'friend works', 'works at', 'at a', 'a circumcision', 'circumcision clinic', 'clinic i', 'i asked', 'asked him', 'him if', 'if he', 'he charges', 'charges alot', 'alot for', 'for his', 'his circumcisions', 'circumcisions he', 'he said', 'said \"', '\" no', 'no ,', ', i', 'i just', 'just keep', 'keep the', 'the tips', 'tips .', '. \"', '\" eos']]\n"
     ]
    }
   ],
   "source": [
    "sentence_bigrams = [bigrams_list(s) for s in sentences]\n",
    "print(sentence_bigrams[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    temp = set(lst2)\n",
    "    lst3 = [value for value in lst1 if value in temp]\n",
    "    return lst3\n",
    "\n",
    "def similarity_score(lst1, lst2):\n",
    "    intersection_len = len(intersection(lst1, lst2))\n",
    "    return (1.0*intersection_len)/len(lst1)#+len(lst2)-intersection_len)\n",
    " \n",
    "def print_closest_sentences(sentence, sentence_bigrams, top_k=3):\n",
    "    bigrams = bigrams_list(sentence)\n",
    "    scores = np.array([similarity_score(bigrams, sbigrams)\n",
    "                       for sbigrams in sentence_bigrams])\n",
    "    top_k_indices = scores.argsort()[-1*top_k:][::-1]\n",
    "    top_k_scores = scores[top_k_indices]\n",
    "    for k in range(top_k):\n",
    "        print(top_k_scores[k], \" -> \", sentences[top_k_indices[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666  ->  sos i bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6666666666666666  ->  sos i bought a pair of shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6666666666666666  ->  sos i recently bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6666666666666666  ->  sos i bought some shoes from my dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos my drug dealer gave me new shoes today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i recently bought some shoes from a drug dealer threedots i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i bought shoes from my drug dealer today i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i recently bought shoes from a drug dealer i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos so i bought some shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.6296296296296297  ->  sos i bought a pair of shoes from a drug dealer threedots i don't know what he laced them with , but i've been tripping all day . eos\n"
     ]
    }
   ],
   "source": [
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 17, 20, 9, 66]\n",
      "sos what do you call a mexican midget ? a paragraph , because he's not a full essay . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph , because he's not a full essay eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph , because he's not a full ese . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph because he's not a full essay . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican midget ? a paragraph , because he is not a full essay . eos\n",
      "0.8947368421052632  ->  sos what do you call a mexican with no legs ? a paragraph , because he's not a full essay . eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph , because he's too short to be an essay . eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph , because he's too short to be an essay . eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph threedots because he's not a full essay eos\n",
      "0.7894736842105263  ->  sos what do you call a mexican midget ? a paragraph , because he is not a full essay ! eos\n",
      "0.7894736842105263  ->  sos what do you call a spanish midget ? a paragraph . because he's not a full essay . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos what do you call\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"sos a woman was arrested when her boyfriend's body was found in a freezer in their living room . who the hell puts a freezer in the living room ? eos\", 'sos what does heroin make you feel like ? more heroin . eos', \"sos why couldn't the physicist understand how boats work ? he thought nothing could possibly travel faster than sea . eos\", \"sos at what age do you tell a highway it's adopted ? eos\", \"sos russians dolls . they're so full of themselves eos\", \"sos how many chocolate bunnies can you put into an empty easter basket ? one . after that the basket won't be empty . eos\", \"sos nurse pops her head into the doctor's office threedots nurse : ' doctor , there's an invisible man in the waiting room . ' doctor : ' tell him i can't see him . ' eos\", 'sos what kind of file makes a hole bigger ? a pedophile eos', \"sos what is a paranoid man's favorite food ? who wants to know ? eos\", \"sos friends invited me to a meteor shower party , but i couldn't make it . they were crushed . eos\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 5, 76, 74, 167]\n",
      "sos what's the difference between being a pedophile and a rapist ? a pedophile comes up to a bar . eos\n",
      "0.4  ->  sos a rapist , a pedophile and a priest walks into a bar . eos\n",
      "0.4  ->  sos a priest , a rapist , and a pedophile walk in to a bar . and he orders a drink . eos\n",
      "0.4  ->  sos what's the difference between a peeping tom and a rapist ? a rapist doesn't waste time beating around the bush eos\n",
      "0.4  ->  sos a priest , a pedophile and a rapist walk into a bar . he orders a drink . eos\n",
      "0.4  ->  sos a rapist , a pedophile and a priest walk into a bar . he orders a drink . eos\n",
      "0.35  ->  sos what's the difference between a slut and a bitch ? a slut fucks everyone threedots a bitch fucks everyone but you . eos\n",
      "0.35  ->  sos what's the difference between a bmw driver and a porcupine ? a porcupine has the pricks on the outside . eos\n",
      "0.35  ->  sos what's the difference between a blonde and a mosquito ? a mosquito will stop sucking after you slap it . eos\n",
      "0.35  ->  sos a priest , a rapist and a pedophile walks into a bar . he sits down and has a drink . eos\n",
      "0.35  ->  sos what's the difference between a vegetarian and a virgin ? a vegetarian doesn't like meat and a virgin doesn't know yet if she likes meat or not . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos what's the difference between being\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 =  load_model('models/checkpoints/jokes_bilstm_gen2.08-4.39.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 107, 1832]\n",
      "sos a guy finds a rectal thermometer in his pocket and says \" i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i recently bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i recently bought shoes from a drug dealer i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos my drug dealer gave me new shoes today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought shoes from my drug dealer today i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos shoes from a drug dealer i bought some shoes from a drug dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i just bought shoes from a drug dealer threedots threedots i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought some shoes from my dealer . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos so i bought some shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n",
      "0.5517241379310345  ->  sos i bought a pair of shoes from a drug dealer today . i don't know what he laced them with , but i've been tripping all day . eos\n"
     ]
    }
   ],
   "source": [
    "joke = generate(model, tokenizer, \"sos a guy finds\", maxlen=40)\n",
    "print(joke)\n",
    "print_closest_sentences(joke, sentence_bigrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
