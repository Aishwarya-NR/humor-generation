{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,Bidirectional\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "EMBEDDING_FILE = \"./data/glove/glove.6B.100d.txt\"\n",
    "TRAIN_DATA_FILE = \"./datasets/jokes.pickle\"\n",
    "VOCABULARY_FILE = \"./datasets/jokes_vocabulary.pickle\"\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 100\n",
    "SEQUENCE_STEP = 1\n",
    "#VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm = 300\n",
    "num_dense = 256\n",
    "rate_drop_lstm = 0.25\n",
    "rate_drop_dense = 0.25\n",
    "\n",
    "act = 'relu'\n",
    "#re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "\n",
    "#Glove Vectors\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training jokes/quotes:  94195\n",
      "#Vocabulary  8338\n"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open(TRAIN_DATA_FILE,\"rb\"))\n",
    "vocabulary = pickle.load(open(VOCABULARY_FILE,\"rb\"))\n",
    "print(\"#training jokes/quotes: \",len(data))\n",
    "print(\"#Vocabulary \",len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check OOV words that are not present in glove vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#non vocabulary words:  108\n",
      "['shouldnt', 'threedots', 'sleevies', 'theyd', 'tifu', 'howd', 'wifes', 'dumbass', 'shitzu', 'kylo', 'biebers', 'theyll', 'youve', 'selfie', 'everyones', 'blowjob', 'draculas', 'harambe', 'tsss', 'schrodingers', 'redditor', 'hadnt', 'werent', 'pubes', 'brexit', 'whered', 'moaner', 'teethbrush', 'idk', 'unfollow', 'mustve', 'theyve', 'cmon', 'hahaha', 'itll', 'cums', 'necrophiliacs', 'downvote', 'shouldve', 'redditors', 'pornstars', 'clickbait', 'whatd', 'tennish', 'cumference', 'neverlands', 'updog', 'nsfw', 'roamin', 'farted', 'jehovahs', 'maam', 'pussies', 'whatll', 'nobodys', 'titties', 'sjws', 'anyones', 'exs', 'beethovens', 'friendzone', 'reddits', 'spaghetto', 'handjob', 'neckbeard', 'wouldve', 'retweet', 'upvotes', 'beastiality', 'crossfitter', 'douchebag', 'hurty', 'motherfucking', 'hodor', 'shits', 'walmarts', 'badum', 'turds', 'reposts', 'subreddit', 'selfies', 'neckbeards', 'pornhub', 'unfollowed', 'tauntaun', 'thatll', 'beiber', 'ofurniture', 'arrr', 'upvote', 'whyd', 'tearable', 'rubbit', 'lmao', 'shitting', 'snackbar', 'somebodys', 'batmans', 'everythings', 'impasta', 'blowjobs', 'cking', 'damnit', 'couldve', 'emoji', 'kermits', 'labracadabrador', 'hahahaha']\n"
     ]
    }
   ],
   "source": [
    "oov = []\n",
    "for word in vocabulary:\n",
    "    if word not in embeddings_index:\n",
    "        oov.append(word)\n",
    "print(\"#non vocabulary words: \",len(oov))\n",
    "print(oov[:max(len(oov),100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing data for faster iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.05\n",
    "data_reduced = data[:int(percentage*len(data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Sentences to create sequence of inetgers and building word to id mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5928 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/keras/preprocessing/text.py:157: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data_reduced)\n",
    "sequences = tokenizer.texts_to_sequences(data_reduced)\n",
    "#word to index\n",
    "word_index = tokenizer.word_index\n",
    "#index to word\n",
    "index_word = {}\n",
    "for k,v in word_index.items():\n",
    "    index_word[v] = k\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(len(sequences),len(data_reduced))\n",
    "assert len(data_reduced) == len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_list = []\n",
    "for sentence in sequences:\n",
    "    word_index_list += sentence\n",
    "print(\"#Total Sequence Length:\", len(word_index_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10\n",
      "nb sequences: 86892\n",
      "(86892, 10) (86892, 5929)\n"
     ]
    }
   ],
   "source": [
    "#create sequences\n",
    "X_train = []\n",
    "y_train = np.zeros((len(word_index_list)-MAX_SEQUENCE_LENGTH,nb_words+1),dtype=np.bool)\n",
    "print(SEQUENCE_STEP,MAX_SEQUENCE_LENGTH)\n",
    "for i in range(0, len(word_index_list) - MAX_SEQUENCE_LENGTH, SEQUENCE_STEP):\n",
    "    X_train.append(word_index_list[i: i + MAX_SEQUENCE_LENGTH])\n",
    "    y_train[i][word_index_list[i + MAX_SEQUENCE_LENGTH]] = 1\n",
    "X_train = np.array(X_train)\n",
    "print('nb sequences:', len(X_train))\n",
    "print(X_train.shape,y_train.shape)\n",
    "assert X_train.shape[0] == y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 72\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)) #Vocabulary size\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 10, 100)           592800    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 600)               962400    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5929)              3563329   \n",
      "=================================================================\n",
      "Total params: 5,118,529\n",
      "Trainable params: 4,525,729\n",
      "Non-trainable params: 592,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = Bidirectional(LSTM(num_lstm, activation=\"relu\",dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm),input_shape=(MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))\n",
    "data_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences= embedding_layer(data_input)\n",
    "x = lstm_layer(embedded_sequences)\n",
    "x = Dropout(rate_drop_dense)(x)\n",
    "preds = Dense(nb_words+1, activation='softmax')(x)\n",
    "model = Model(inputs=[data_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer='rmsprop',\n",
    "        metrics=[categorical_accuracy])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_lstm_glove_vectors_0.25_0.25\n"
     ]
    }
   ],
   "source": [
    "STAMP = 'simple_lstm_glove_vectors_%.2f_%.2f'%(rate_drop_lstm,rate_drop_dense)\n",
    "print(STAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping =EarlyStopping(monitor='val_loss', patience=2)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78202 samples, validate on 8690 samples\n",
      "Epoch 1/5\n",
      "78202/78202 [==============================] - 24s 304us/step - loss: 6.0714 - categorical_accuracy: 0.1360 - val_loss: 5.7913 - val_categorical_accuracy: 0.1551\n",
      "Epoch 2/5\n",
      "78202/78202 [==============================] - 18s 236us/step - loss: 5.5932 - categorical_accuracy: 0.1683 - val_loss: 5.5814 - val_categorical_accuracy: 0.1755\n",
      "Epoch 3/5\n",
      "78202/78202 [==============================] - 18s 236us/step - loss: 5.3841 - categorical_accuracy: 0.1852 - val_loss: 5.4643 - val_categorical_accuracy: 0.1885\n",
      "Epoch 4/5\n",
      "78202/78202 [==============================] - 18s 236us/step - loss: 5.2354 - categorical_accuracy: 0.1973 - val_loss: 5.3614 - val_categorical_accuracy: 0.1953\n",
      "Epoch 5/5\n",
      "78202/78202 [==============================] - 18s 236us/step - loss: 5.1283 - categorical_accuracy: 0.2032 - val_loss: 5.3111 - val_categorical_accuracy: 0.2009\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train,y_train, \\\n",
    "        epochs=5, batch_size=256, shuffle=True, \\\n",
    "         callbacks=[early_stopping, model_checkpoint],\n",
    "                validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.311095283824363\n"
     ]
    }
   ],
   "source": [
    "print(bst_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 13,\n",
       " 30,\n",
       " 4,\n",
       " 476,\n",
       " 57,\n",
       " 6,\n",
       " 4,\n",
       " 791,\n",
       " 122,\n",
       " 53,\n",
       " 4080,\n",
       " 112,\n",
       " 1815,\n",
       " 40,\n",
       " 118,\n",
       " 12,\n",
       " 20,\n",
       " 9,\n",
       " 374,\n",
       " 2]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict([X_train[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(predict,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 17,  3,  3,  2,  2,  2,  2,  4,  2])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "was\n",
      "a\n",
      "a\n",
      "eos\n",
      "eos\n",
      "eos\n",
      "eos\n",
      "the\n",
      "eos\n"
     ]
    }
   ],
   "source": [
    "for id in idx:\n",
    "    print(index_word[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
