{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved -1 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Link to original dataset: https://github.com/CrowdTruth/Short-Text-Corpus-For-Humor-Detection/blob/master/datasets/humorous_oneliners.pickle\n",
    "# git versions changed the Unix new lines ('\\n') to DOS lines ('\\r\\n') so the following step is needed\n",
    "\n",
    "from library.dos2unixfile import dos2unixfile\n",
    "dos2unixfile(\"datasets/original_humorous_oneliners.pickle\", \"datasets/humorous_oneliners.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataL = pd.read_pickle(\"datasets/humorous_oneliners.pickle\", compression=None)\n",
    "jokes_data = pd.DataFrame(jokes_dataL, columns = [\"joke\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insanity is hereditary, - You get it from your children.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An honest politician is one who, when bought, stays bought.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You can tune a piano, but you can't tuna fish.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A closed mouth gathers no foot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What's black and white and red all over?  An embarassed zebra.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What's black and white and red all over?  Certainly not the Halifax newspapers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gravity doesn't exist: the earth sucks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What's the most popular form of birth control?  The headache.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>He who laughs last probably doesn't understand the joke.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Let him who takes the plunge remember to return it by Tuesday.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              joke\n",
       "0  Insanity is hereditary, - You get it from your children.                       \n",
       "1  An honest politician is one who, when bought, stays bought.                    \n",
       "2  You can tune a piano, but you can't tuna fish.                                 \n",
       "3  A closed mouth gathers no foot.                                                \n",
       "4  What's black and white and red all over?  An embarassed zebra.                 \n",
       "5  What's black and white and red all over?  Certainly not the Halifax newspapers.\n",
       "6  Gravity doesn't exist: the earth sucks.                                        \n",
       "7  What's the most popular form of birth control?  The headache.                  \n",
       "8  He who laughs last probably doesn't understand the joke.                       \n",
       "9  Let him who takes the plunge remember to return it by Tuesday.                 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "jokes_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jokes originally:  (5251, 1)\n",
      "Number of jokes after cleaning:  (5218, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insanity is hereditary, - you get it from your children.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>an honest politician is one who, when bought, stays bought.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you can tune a piano, but you can't tuna fish.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a closed mouth gathers no foot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what's black and white and red all over? an embarassed zebra.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>what's black and white and red all over? certainly not the halifax newspapers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gravity doesn't exist: the earth sucks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what's the most popular form of birth control? the headache.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>he who laughs last probably doesn't understand the joke.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>let him who takes the plunge remember to return it by tuesday.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             joke\n",
       "0  insanity is hereditary, - you get it from your children.                      \n",
       "1  an honest politician is one who, when bought, stays bought.                   \n",
       "2  you can tune a piano, but you can't tuna fish.                                \n",
       "3  a closed mouth gathers no foot.                                               \n",
       "4  what's black and white and red all over? an embarassed zebra.                 \n",
       "5  what's black and white and red all over? certainly not the halifax newspapers.\n",
       "6  gravity doesn't exist: the earth sucks.                                       \n",
       "7  what's the most popular form of birth control? the headache.                  \n",
       "8  he who laughs last probably doesn't understand the joke.                      \n",
       "9  let him who takes the plunge remember to return it by tuesday.                "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data preprocessing\n",
    "\n",
    "MIN_JOKE_LEN = 10\n",
    "MAX_JOKE_LEN = 200\n",
    "\n",
    "def regex_match(sentence):\n",
    "    return bool(re.match(r\"^[ -z]+$\", sentence))\n",
    "\n",
    "print(\"Number of jokes originally: \", jokes_data.shape)\n",
    "\n",
    "# convert to lowercase. Strip left and right whitespaces. Replace multiple spaces with a single space\n",
    "# Replace odd characters with appropriate ones\n",
    "\n",
    "clean_jokes_data = (jokes_data\n",
    "                    .replace('^\\-', '', regex=True)\n",
    "                    .replace('[\\t|\\r|\\n]', ' ', regex=True)\n",
    "                    .replace(\"\\\\\\\\\", ' ', regex=True)\n",
    "                    .replace(\"`|´\", \"'\", regex=True)\n",
    "                    .replace(\"\\u2018|\\u2019\", \"'\", regex=True)\n",
    "                    .replace(\"\\u201C|\\u201D\", '\"', regex=True)\n",
    "                    .replace(\"–\", '-', regex=True)\n",
    "                    .replace(\"…\", '...', regex=True)\n",
    "                    .replace(\"\\&\", \" and \", regex=True)\n",
    "                    .replace(\"\\s+\", ' ', regex=True)\n",
    "                    .applymap(str.lower)\n",
    "                    .applymap(str.strip))\n",
    "\n",
    "clean_jokes_data = clean_jokes_data[clean_jokes_data[\"joke\"].apply(regex_match) &\n",
    "                                    (clean_jokes_data[\"joke\"].str.len() >= MIN_JOKE_LEN) & \n",
    "                                    (clean_jokes_data[\"joke\"].str.len() <= MAX_JOKE_LEN)]\n",
    "\n",
    "print(\"Number of jokes after cleaning: \", clean_jokes_data.shape)\n",
    "clean_jokes_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', '$', '%', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Number of unique chars =  58\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(clean_jokes_data[\"joke\"].str.cat(sep=''))))\n",
    "print(unique_chars)\n",
    "print(\"Number of unique chars = \", len(unique_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to and fro mapping from characters to indices\n",
    "START_INDEX = 0\n",
    "END_INDEX = len(unique_chars) + 1\n",
    "char_indices = dict((c, i+1) for i, c in enumerate(unique_chars))\n",
    "indices_char = dict((i+1, c) for i, c in enumerate(unique_chars))\n",
    "n_letters = len(unique_chars) + 2 # Plus SOS & EOS markers (start and end of sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa4f8349dd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [START_INDEX] + [to_ix[w] for w in seq] + [END_INDEX]\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size, batch_size, use_gpu):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        if self.use_gpu:\n",
    "            h0 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim).cuda())\n",
    "            c0 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "            c0 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.char_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), self.batch_size, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU =  True\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = torch.cuda.is_available()\n",
    "print(\"Use GPU = \", USE_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insanity is hereditary, - you get it from your children.\n",
      "{'t': 52, 'a': 33, '8': 24, '=': 28, 'o': 47, 'j': 42, '0': 16, \"'\": 7, 'x': 56, 'm': 45, '\"': 3, 'y': 57, '-': 13, ')': 9, 'l': 44, '(': 8, '_': 32, 'p': 48, 'r': 50, 'b': 34, '^': 31, '#': 4, 'q': 49, ';': 27, 'z': 58, '*': 10, '!': 2, 'k': 43, 'n': 46, '/': 15, '9': 25, 'v': 54, 'u': 53, '@': 30, '$': 5, '+': 11, ':': 26, '2': 18, '4': 20, '%': 6, '7': 23, '1': 17, 'g': 39, '.': 14, '3': 19, 's': 51, 'h': 40, 'w': 55, ' ': 1, '?': 29, '5': 21, 'c': 35, 'f': 38, '6': 22, ',': 12, 'i': 41, 'd': 36, 'e': 37}\n"
     ]
    }
   ],
   "source": [
    "# testing prepare_sequence\n",
    "joke = clean_jokes_data[\"joke\"].iloc[0]\n",
    "inputs = prepare_sequence(list(joke), char_indices)\n",
    "print(joke)\n",
    "print(char_indices)\n",
    "#print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 64\n",
    "BATCH_SIZE = 1\n",
    "VOCAB_SIZE = n_letters\n",
    "model = CharLSTM(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, VOCAB_SIZE, BATCH_SIZE, USE_GPU)\n",
    "if USE_GPU:\n",
    "    model = model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.0980, -3.9963, -4.2156,  ..., -4.1599, -4.0525, -4.0420],\n",
      "        [-4.0825, -3.9655, -4.1046,  ..., -4.1561, -4.0493, -4.1139],\n",
      "        [-4.1239, -4.0883, -4.1711,  ..., -4.0700, -3.9779, -4.1173],\n",
      "        ...,\n",
      "        [-4.0763, -4.1162, -4.1881,  ..., -4.0598, -3.9594, -4.1122],\n",
      "        [-4.0773, -4.1022, -4.1655,  ..., -4.2042, -4.0117, -4.1081],\n",
      "        [-4.0384, -4.0719, -4.0964,  ..., -4.1367, -3.9341, -4.1614]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(list(joke), char_indices)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from livelossplot import PlotLosses\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "NUM_EPOCHS = 25\n",
    "# prepare training data in correct form\n",
    "jokes_to_chars = [list(joke) for joke in clean_jokes_data[\"joke\"].values.tolist()]\n",
    "jokes_to_seq = [prepare_sequence(joke, char_indices) for joke in jokes_to_chars]\n",
    "training_data = [(seq[:-1], seq[1:]) for seq in jokes_to_seq]\n",
    "if USE_GPU:\n",
    "    training_data = [(Variable(x.cuda()), Variable(y.cuda())) for x,y in training_data]\n",
    "else:\n",
    "    training_data = [(Variable(x), Variable(y)) for x,y in training_data]\n",
    "\n",
    "#training_data = training_data[:100]\n",
    "#print(training_data)\n",
    "\n",
    "liveloss = None\n",
    "train_loss = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"epoch#\", epoch+1)\n",
    "    \n",
    "    # Shuffle training data\n",
    "    random.shuffle(training_data)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for sentence_in, sentence_out in tqdm(training_data):\n",
    "        \n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, sentence_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data[0]\n",
    "    avg_loss = total_loss/len(training_data)\n",
    "    train_loss.append(avg_loss)\n",
    "    \n",
    "    if liveloss is None:\n",
    "        liveloss = PlotLosses()\n",
    "    liveloss.update({\n",
    "        'training loss': avg_loss\n",
    "    })\n",
    "    liveloss.draw()\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Total elapsed time: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
